diff --git a/src/peft/tuners/lora.py b/src/peft/tuners/lora.py
index a8003df..81522ec 100644
--- a/src/peft/tuners/lora.py
+++ b/src/peft/tuners/lora.py
@@ -39,7 +39,7 @@ from ..utils import (
     transpose,
 )
 from .tuners_utils import BaseTuner, BaseTunerLayer
-
+from nncf import torch as nncf_torch
 
 if is_bnb_available():
     import bitsandbytes as bnb
@@ -155,7 +155,10 @@ class LoraLayer(BaseTunerLayer):
             self.scaling[adapter_name] = lora_alpha / r
         if init_lora_weights:
             self.reset_lora_parameters(adapter_name)
-        self.to(self.weight.device)
+        if isinstance(self, Linear):
+            self.to(self.self_linear.weight.device)
+        else:
+            self.to(self.weight.device)
 
     def update_layer_conv2d(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights):
         self.r[adapter_name] = r
@@ -374,12 +377,19 @@ class LoraModel(BaseTuner):
 
     @staticmethod
     def _replace_module(parent, child_name, new_module, child):
+        if isinstance(child, Linear):
+            child = child.self_linear
         setattr(parent, child_name, new_module)
-        new_module.weight = child.weight
+        if isinstance(new_module, Linear):
+            new_module.self_linear.weight = child.weight
+        else:
+            new_module.weight = child.weight
         if hasattr(child, "bias"):
             if child.bias is not None:
-                new_module.bias = child.bias
-
+                if isinstance(new_module, Linear):
+                    new_module.self_linear.bias = child.bias
+                else:
+                    new_module.bias = child.bias
         if getattr(child, "state", None) is not None:
             new_module.state = child.state
             new_module.to(child.weight.device)
@@ -577,6 +587,12 @@ class LoraModel(BaseTuner):
                         padding=target.padding,
                         dilation=target.dilation,
                     )
+                elif isinstance(target, Linear):
+                    bias = target.self_linear.bias is not None
+                    if getattr(target.self_linear, "is_target_conv_1d_layer", False):
+                        new_module = Conv1D(target.self_linear.out_features, target.self_linear.in_features)
+                    else:
+                        new_module = torch.nn.Linear(target.self_linear.in_features, target.self_linear.out_features, bias=bias)
                 else:
                     bias = target.bias is not None
                     if getattr(target, "is_target_conv_1d_layer", False):
@@ -831,7 +847,17 @@ class LoraModel(BaseTuner):
 #  ------------------------------------------------------------------------------------------
 
 
-class Linear(nn.Linear, LoraLayer):
+@nncf_torch.register_module()
+class SelfLinear(nn.Linear):
+    def __init__(self, in_features: int, out_features: int, fan_in_fan_out, **kwargs):
+        nn.Linear.__init__(self, in_features, out_features, **kwargs)
+        self.fan_in_fan_out = fan_in_fan_out
+
+    def forward(self, input: torch.Tensor) -> torch.Tensor:
+        return F.linear(input, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)
+
+
+class Linear(nn.Module, LoraLayer):
     # Lora implemented in a dense layer
     def __init__(
         self,
@@ -847,16 +873,20 @@ class Linear(nn.Linear, LoraLayer):
     ):
         init_lora_weights = kwargs.pop("init_lora_weights", True)
 
-        nn.Linear.__init__(self, in_features, out_features, **kwargs)
+        nn.Module.__init__(self)
         LoraLayer.__init__(self, in_features=in_features, out_features=out_features)
+
+        # for nncf
+        self.self_linear = SelfLinear(in_features, out_features, fan_in_fan_out, **kwargs)
         # Freezing the pre-trained weight matrix
-        self.weight.requires_grad = False
+        self.self_linear.weight.requires_grad = False
 
         self.fan_in_fan_out = fan_in_fan_out
         if fan_in_fan_out:
             self.weight.data = self.weight.data.T
+            self.self_linear.weight.data = self.self_linear.weight.data.T
 
-        nn.Linear.reset_parameters(self)
+        nn.Linear.reset_parameters(self.self_linear)
         self.update_layer(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights)
         self.active_adapter = adapter_name
         self.is_target_conv_1d_layer = is_target_conv_1d_layer
@@ -868,7 +898,7 @@ class Linear(nn.Linear, LoraLayer):
             warnings.warn("Already merged. Nothing to do.")
             return
         if self.r[self.active_adapter] > 0:
-            self.weight.data += self.get_delta_weight(self.active_adapter)
+            self.self_linear.weight.data += self.get_delta_weight(self.active_adapter)
             self.merged = True
 
     def unmerge(self):
@@ -879,6 +909,7 @@ class Linear(nn.Linear, LoraLayer):
             return
         if self.r[self.active_adapter] > 0:
             self.weight.data -= self.get_delta_weight(self.active_adapter)
+            self.self_linear.weight.data -= self.get_delta_weight(self.active_adapter)
             self.merged = False
 
     def get_delta_weight(self, adapter):
@@ -893,13 +924,13 @@ class Linear(nn.Linear, LoraLayer):
     def forward(self, x: torch.Tensor):
         previous_dtype = x.dtype
         if self.active_adapter not in self.lora_A.keys():
-            return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)
+            return self.self_linear(x)
         if self.disable_adapters:
             if self.r[self.active_adapter] > 0 and self.merged:
                 self.unmerge()
-            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)
+            result = self.self_linear(x)
         elif self.r[self.active_adapter] > 0 and not self.merged:
-            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)
+            result = self.self_linear(x)
 
             x = x.to(self.lora_A[self.active_adapter].weight.dtype)
 
@@ -910,7 +941,7 @@ class Linear(nn.Linear, LoraLayer):
                 * self.scaling[self.active_adapter]
             )
         else:
-            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)
+            result = self.self_linear(x)
 
         result = result.to(previous_dtype)
 
