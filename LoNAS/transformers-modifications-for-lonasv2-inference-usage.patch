diff --git a/src/transformers/modeling_utils.py b/src/transformers/modeling_utils.py
index 45459ed22..226c55a4e 100644
--- a/src/transformers/modeling_utils.py
+++ b/src/transformers/modeling_utils.py
@@ -3201,6 +3201,31 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
                 if param_device_map[p] == "disk"
             }
 
+        def module_reshape(state_dict):
+            for param_name, param in state_dict.items():
+                tensor_name = param_name
+                splits = tensor_name.split(".")
+                if len(splits) > 1:
+                    module = model_to_load
+                    parent = None
+                    for split in splits[:-1]:
+                        new_module = getattr(module, split)
+                        if new_module is None:
+                            raise ValueError(f"{module} has no attribute {split}.")
+                        parent = module
+                        module = new_module
+                    tensor_name = splits[-1]
+                    old_value = getattr(module, tensor_name)
+                    if old_value.shape != param.shape and isinstance(module, nn.Linear):
+                        new_module = torch.nn.Linear(
+                            param.shape[1],
+                            param.shape[0],
+                            bias=module.bias is not None,
+                            dtype=module.weight.dtype,
+                            device=module.weight.device
+                        )
+                        setattr(parent, splits[-2], new_module)
+
         if state_dict is not None:
             # Whole checkpoint
             mismatched_keys = _find_mismatched_keys(
@@ -3211,6 +3236,7 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
                 remove_prefix_from_model,
                 ignore_mismatched_sizes,
             )
+            module_reshape(state_dict)
             error_msgs = _load_state_dict_into_model(model_to_load, state_dict, start_prefix)
             offload_index = None
         else:
@@ -3256,6 +3282,8 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
                     ignore_mismatched_sizes,
                 )
 
+                module_reshape(state_dict)
+
                 if low_cpu_mem_usage:
                     new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
                         model_to_load,
diff --git a/src/transformers/models/llama/modeling_llama.py b/src/transformers/models/llama/modeling_llama.py
index 5709fc6a7..82a323cf7 100644
--- a/src/transformers/models/llama/modeling_llama.py
+++ b/src/transformers/models/llama/modeling_llama.py
@@ -306,9 +306,9 @@ class LlamaAttention(nn.Module):
             key_states = self.k_proj(hidden_states)
             value_states = self.v_proj(hidden_states)
 
-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
 
         kv_seq_len = key_states.shape[-2]
         if past_key_value is not None:
@@ -318,8 +318,9 @@ class LlamaAttention(nn.Module):
 
         if past_key_value is not None:
             # reuse k, v, self_attention
-            key_states = torch.cat([past_key_value[0], key_states], dim=2)
-            value_states = torch.cat([past_key_value[1], value_states], dim=2)
+            num_head = key_states.size(1)
+            key_states = torch.cat([past_key_value[0][:, :num_head], key_states], dim=2)
+            value_states = torch.cat([past_key_value[1][:, :num_head], value_states], dim=2)
 
         past_key_value = (key_states, value_states) if use_cache else None
 
@@ -329,9 +330,9 @@ class LlamaAttention(nn.Module):
 
         attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)
 
-        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
+        if attn_weights.size(0) != bsz or attn_weights.size(2) != q_len or attn_weights.size(3) != kv_seq_len:
             raise ValueError(
-                f"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is"
+                f"Attention weights should be of size {(bsz, -1, q_len, kv_seq_len)}, but is"
                 f" {attn_weights.size()}"
             )
 
@@ -346,14 +347,14 @@ class LlamaAttention(nn.Module):
         attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
         attn_output = torch.matmul(attn_weights, value_states)
 
-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
+        if attn_output.size(0) != bsz or attn_output.size(2) != q_len or attn_output.size(3) != self.head_dim:
             raise ValueError(
-                f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
+                f"`attn_output` should be of size {(bsz, -1, q_len, self.head_dim)}, but is"
                 f" {attn_output.size()}"
             )
 
         attn_output = attn_output.transpose(1, 2).contiguous()
-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
+        attn_output = attn_output.reshape(bsz, q_len, -1)
 
         if self.pretraining_tp > 1:
             attn_output = attn_output.split(self.hidden_size // self.pretraining_tp, dim=2)
