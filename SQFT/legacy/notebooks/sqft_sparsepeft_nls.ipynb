{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Low-cost Model Adaptation in Low-precision Sparse Foundation Models: **SQFT + SparsePEFT (NLS)** üöÄ"
   ],
   "metadata": {
    "id": "Yt3cghlzcITV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Welcome to an exhilarating journey as we delve into the realm of fine-tuning for efficient large language models (LLMs)! üåü\n",
    "\n",
    "We'll be working with Apple's OpenELM model and optimizing it for a specific task with one of our SQFT pipelines: **SQFT + SparsePEFT (NLS)**."
   ],
   "metadata": {
    "id": "7HJgBi6Fcc3z"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All of our SQFT notebooks:\n",
    "\n",
    "- **SQFT (LoRA)**: [link]()\n",
    "- **SQFT (NLS)**: [link]()\n",
    "- **SQFT + SparsePEFT (LoRA)**: [link]()\n",
    "- **SQFT + SparsePEFT (NLS)**: [link]()\n",
    "- **SQFT + QA-SparsePEFT (LoRA)**: [link]()\n",
    "- **SQFT + QA-SparsePEFT (NLS)**: [link]()"
   ],
   "metadata": {
    "id": "mPWRSU2emi5d"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Note: This notebook introduces the use of this pipeline with NLS. For a simpler understanding of SQFT, you can start with some simple experiments using the twin notebook [SQFT + SparsePEFT (LoRA)](). NLS enhances the performance of sparse or quantized models beyond what LoRA offers.\n",
    "  "
   ],
   "metadata": {
    "id": "G08Q4farXiqI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Overview\n",
    "\n",
    "In this notebook, you will learn a practical and novel solution that generates efficient (**sparse** or **quantized**) models fine-tuned for downstream-specific tasks for real-world applications. The notebook introduces the solution of SQFT + SparsePEFT (NLS), covering the following key points:\n",
    "\n",
    "1. Setup the Environment ‚öôÔ∏è\n",
    "2. Sparsification ‚úÇÔ∏è\n",
    "3. Load Model üöÄ\n",
    "4. Test the Model Before Tuning üß™\n",
    "5. Configure the LoRA Settings üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
    "6. Configure NNCF For NLS üõ†Ô∏è\n",
    "7. Prepare the Dataset üìö\n",
    "8. Finetune the Model üéØ\n",
    "9. Extract the heuristic sub-adapter üîß\n",
    "10. Search an optimal sub-adapter (Optional) üîç\n",
    "11. Merge the Model üß©\n",
    "12. Evaluate the Finetuned Model üèÜ\n",
    "\n",
    "This notebook illustrates how fine-tuning can significantly boost the performance of a sparse model across a diverse array of topics, enhancing its versatility and applicability to various domains. You will gain valuable insights into the process of developing a **task-specific** highly-efficient model capable of delivering accurate and relevant responses to a broad spectrum of questions."
   ],
   "metadata": {
    "id": "CUJYLcWKdIdF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Quick Start"
   ],
   "metadata": {
    "id": "ryxZOqMzrADr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 1: Setup the Environment ‚öôÔ∏è\n",
    "\n",
    "Let's start by setting up our environment! We'll install all the essential packages, including the Hugging Face `transformers` library, `peft` library, and a few additional tools. üì¶\n",
    "Please follow https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning/tree/main/SQFT#setup to set up the environment for SQFT."
   ],
   "metadata": {
    "id": "mR5_H528fBzc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "üëÄ Check whether GPU is available:"
   ],
   "metadata": {
    "id": "Rc5uPuwijnxA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU is available. Using GPU.\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ],
   "metadata": {
    "id": "mUdnX7Yfj9mZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "üîë Logging into Hugging Face Hub:"
   ],
   "metadata": {
    "id": "Y0SffLEy-q_G"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19af72a5-9b23-4fe5-8ca8-bf45c7bdf7dc"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 2: Sparsification ‚úÇÔ∏è\n",
    "\n",
    "Before fine-tuning, SQFT employs a simple but effective pruning approach [Wanda](https://arxiv.org/abs/2306.11695) to\n",
    "sparsify the language model, serving as the base model (frozen) for adapter training.\n",
    "Clone the Wanda repo and apply our patch:"
   ],
   "metadata": {
    "id": "qJAqPaMhkidX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/locuslab/wanda.git && cd wanda && git checkout 8e8fc87 && git apply --ignore-space-change --ignore-whitespace ../wanda-modifications-for-sqft-usage.patch"
   ],
   "metadata": {
    "id": "TT0mOOvflPWQ",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below is the command for unstructured sparsifying [apple/OpenELM-1_1B](https://huggingface.co/apple/OpenELM-1_1B)\n",
    "with Wanda, to achieve unstructured 50% sparsity. Please note that we retain the model in FP32 precision to maximize its performance, and the tokenizer used by the OpenELM model is the one from [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)."
   ],
   "metadata": {
    "id": "MJaDTrYHlwMZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!python wanda/main.py --model apple/OpenELM-1_1B --dtype auto --tokenizer meta-llama/Llama-2-7b-hf --prune_method wanda --sparsity_ratio 0.5 --sparsity_type unstructured --save wanda_out --save_model sqft-openelm-1_1b-50-base"
   ],
   "metadata": {
    "id": "nSAocqMkmF4L",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "- `--model`: The identifier for the model on the Hugging Face model hub or local path.\n",
    "- `--sparsity_ratio`: Specifies the percentage of weights to be pruned.\n",
    "- `--save_model`: Specifies the directory where the pruned language model will be stored.\n",
    "\n",
    "Further details can be referred to [Wanda](https://github.com/locuslab/wanda).\n",
    "Note that the sparsifying step can be replaced by other sparse algorithms.\n",
    "Feel free to try other pruning approaches for the base model before training. üòä"
   ],
   "metadata": {
    "id": "h8qV71jemkOf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 3: Load Model üöÄ\n",
    "\n",
    "Let's get started by loading the sparsified OpenELM model with Hugging Face's `AutoModelForCausalLM` class. We'll also bring in the corresponding tokenizer to handle our input data preprocessing. üõ†Ô∏è"
   ],
   "metadata": {
    "id": "3jg9ssIkqF2M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"sqft-openelm-1_1b-50-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Set padding side to the right to ensure proper attention masking during fine-tuning\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(device)\n",
    "# Disable caching mechanism to reduce memory usage during fine-tuning\n",
    "model.config.use_cache = False"
   ],
   "metadata": {
    "id": "RFZl1xcPqcEn",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 4: Test the Model Before Tuning üß™\n",
    "\n",
    "Before diving into fine-tuning, let's first evaluate the out-of-the-box performance of the **sparsified** OpenELM model. We will use [lm-evaluation-harness v0.4.2](https://github.com/EleutherAI/lm-evaluation-harness/tree/v0.4.2) to assess the model on the `ARC-Easy` dataset with the default configuration. In the subsequent fine-tuning steps, we will also use the training set of `ARC-Easy` to observe the performance changes before and after fine-tuning. Please note that this notebook is intended to demonstrate the usage of SQFT, so a small dataset is used here for demonstration."
   ],
   "metadata": {
    "id": "BcdCZU1XAa82"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!lm_eval --model hf --model_args pretrained=sqft-openelm-1_1b-50-base,add_bos_token=True,trust_remote_code=True --tasks arc_easy --batch_size auto:4"
   ],
   "metadata": {
    "id": "a38j4NRbAaql"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can get the following results:\n",
    "\n",
    "| Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
    "|--------|------:|------|-----:|--------|-----:|---|-----:|\n",
    "|arc_easy|      1|none  |     0|acc     |0.4966|¬±  |0.0103|\n",
    "|        |       |none  |     0|acc_norm|0.4415|¬±  |0.0102|\n"
   ],
   "metadata": {
    "id": "Sooc2g6R6Wjp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 5: Configure the LoRA Settings üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
    "\n",
    "To efficiently finetune our model, we'll leverage the LoRA (Low-Rank Adaptation) technique.\n",
    "LoRA enables us to tailor the model to our specific task by training only a small subset of additional parameters. This significantly reduces on training time and memory usage! ‚è∞\n",
    "\n",
    "We'll set up the LoRA configuration by specifying the low rank (r) and the target modules we aim to adapt. For the SQFT + SparsePEFT solution (which is what this notebook introduces), we need to set `sparse_adapter` to `True` in order to make the adapter sparse, so that the adapter can be merged into the base model without any loss of sparsity. üéØ"
   ],
   "metadata": {
    "id": "nNarvwDdrbXG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare the model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"qkv_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    sparse_adapter=True, # SparsePEFT\n",
    ")\n",
    "\n",
    "# Apply the LoRA configuration to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ],
   "metadata": {
    "id": "SycK3eAUrpHq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 6: Configure NNCF For NLS üõ†Ô∏è\n",
    "\n",
    "To enhance performance, SQFT leverages Neural Low-rank Adapter Search (NLS) to identify the optimal adapter configuration, surpassing the traditional LoRA approach. üöÄ Alternatively, you can skip this step and refer to the [SQFT + SparsePEFT (LoRA)]() notebook to conduct some preliminary experiments with LoRA.\n",
    "\n",
    "The following code demonstrates how to configure and create a neural network model with elastic low-rank support using the Neural Network Compression Framework (NNCF). Specifically, this code defines an NNCF configuration dictionary, sets up input information and BootstrapNAS (Neural Architecture Search) training parameters, including the progressive shrinking algorithm and width elasticity. üìä It then creates an NNCF network based on the configuration dictionary and applies compression control and model compression algorithms. üîß\n"
   ],
   "metadata": {
    "id": "RbfpTaBruDOT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from nncf import NNCFConfig\n",
    "from nncf.experimental.torch.nas.bootstrapNAS.training.model_creator_helpers import create_compressed_model_from_algo_names\n",
    "from nncf.torch.model_creation import create_nncf_network\n",
    "\n",
    "rank_search_space = [16, 12, 8]\n",
    "\n",
    "# Define the NNCF configuration dictionary, including input information and BootstrapNAS training parameters\n",
    "nncf_config_dict = {\n",
    "    \"input_info\": [\n",
    "        {\n",
    "            \"sample_size\": [1, 256],\n",
    "            \"type\": \"long\",\n",
    "            \"keyword\": \"input_ids\"\n",
    "        },\n",
    "        {\n",
    "            \"sample_size\": [1, 256],\n",
    "            \"type\": \"long\",\n",
    "            \"keyword\": \"attention_mask\"\n",
    "        }\n",
    "    ],\n",
    "    \"bootstrapNAS\": {\n",
    "        \"training\": {\n",
    "            \"algorithm\": \"progressive_shrinking\",\n",
    "            \"frozen_layers_allowed\": True,\n",
    "            \"progressivity_of_elasticity\": [\"width\"],\n",
    "            \"batchnorm_adaptation\": {\n",
    "                \"num_bn_adaptation_samples\": 0\n",
    "            },\n",
    "            \"schedule\": {\n",
    "                \"list_stage_descriptions\": [\n",
    "                    {\"train_dims\": [\"width\"], \"epochs\": 3, \"depth_indicator\": 1, \"width_indicator\": 5, \"init_lr\": 1e-4, \"epochs_lr\": 3, \"sample_rate\": 1}\n",
    "                ]\n",
    "            },\n",
    "            \"elasticity\": {\n",
    "                \"available_elasticity_dims\": [\"width\"],\n",
    "                \"width\": {\n",
    "                    \"overwrite_groups\": [\n",
    "                        [\n",
    "                            \"{re}PeftModelForCausalLM/LoraModel[base_model]/OpenELMForCausalLM[model]/OpenELMModel[transformer]/ModuleList[layers]/OpenELMDecoderLayer[{*}]/OpenELMMultiHeadCausalAttention[attn]/Linear[qkv_proj]/ModuleDict[lora_A]/NNCFLinear[default]/linear_0\"\n",
    "                        ]\n",
    "                    ],\n",
    "                    \"overwrite_groups_widths\": [\n",
    "                        rank_search_space # the search space of low rank (r)\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Process the overwrite groups and their corresponding widths\n",
    "base_overwrite_groups = nncf_config_dict[\"bootstrapNAS\"][\"training\"][\"elasticity\"][\"width\"][\"overwrite_groups\"]\n",
    "base_overwrite_groups_widths = nncf_config_dict[\"bootstrapNAS\"][\"training\"][\"elasticity\"][\"width\"][\"overwrite_groups_widths\"]\n",
    "overwrite_groups, overwrite_groups_widths = [], []\n",
    "num_layers = model.config.num_transformer_layers\n",
    "for group, width in zip(base_overwrite_groups, base_overwrite_groups_widths):\n",
    "    cur_search_space = width\n",
    "    if group[0].startswith(\"{re}\"):\n",
    "        new_group = [[item.replace(\"{re}\", \"\").replace(\"{*}\", str(i)) for item in group] for i in range(num_layers)]\n",
    "        new_width = [cur_search_space for _ in range(num_layers)]\n",
    "    else:\n",
    "        new_group = [group]\n",
    "        new_width = [cur_search_space]\n",
    "    overwrite_groups.extend(new_group)\n",
    "    overwrite_groups_widths.extend(new_width)\n",
    "\n",
    "# Update the configuration dictionary with the processed overwrite groups and widths\n",
    "nncf_config_dict[\"bootstrapNAS\"][\"training\"][\"elasticity\"][\"width\"][\"overwrite_groups\"] = overwrite_groups\n",
    "nncf_config_dict[\"bootstrapNAS\"][\"training\"][\"elasticity\"][\"width\"][\"overwrite_groups_widths\"] = overwrite_groups_widths\n",
    "nncf_config = NNCFConfig.from_dict(nncf_config_dict)\n",
    "\n",
    "# Create the NNCF network and apply compression control and model compression algorithms\n",
    "nncf_network = create_nncf_network(model, nncf_config)\n",
    "compression_ctrl, model = create_compressed_model_from_algo_names(\n",
    "    nncf_network, nncf_config, algo_names=[\"progressive_shrinking\"]\n",
    ")"
   ],
   "metadata": {
    "id": "tWB_8_Xtt9-F"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 7: Prepare the Dataset üìö\n",
    "\n",
    "We prepare a domain-specific dataset to finetune our sparsified model.\n",
    "As tested in step 4, to introduce our process more simply, we will utilize a small dataset, `ARC-Easy`. This dataset features authentic grade-school level, multiple-choice science questions, designed to foster research in advanced question-answering. By honing in on the question-answer pairs, we aim to adapt our model to deliver precise and relevant responses to a variety of inquiries. üîç"
   ],
   "metadata": {
    "id": "gNqfEq5I5jMJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "üîß Loading Data and Preparing Prompts:"
   ],
   "metadata": {
    "id": "uNC-T-BMEyzm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "\"\"\"\n",
    "This function is inspired by the implementation in `lm-evaluation-harness` library.\n",
    "It processes the ARC-Easy dataset to create prompts for question-answering tasks.\n",
    "\"\"\"\n",
    "def add_prompt_func_arc(doc):\n",
    "\n",
    "    def _process_doc(doc):\n",
    "        \"\"\"\n",
    "        Process a single document to convert numeric answer keys to letters and\n",
    "        format the document into a dictionary.\n",
    "        \"\"\"\n",
    "        # Map numeric answer keys to letters\n",
    "        num_to_letter = {\"1\": \"A\", \"2\": \"B\", \"3\": \"C\", \"4\": \"D\", \"5\": \"E\"}\n",
    "        doc[\"answerKey\"] = num_to_letter.get(doc[\"answerKey\"], doc[\"answerKey\"])\n",
    "\n",
    "        # Create a dictionary with necessary fields\n",
    "        out_doc = {\n",
    "            \"id\": doc[\"id\"],\n",
    "            \"query\": \"Question: \" + doc[\"question\"] + \"\\nAnswer:\",\n",
    "            \"choices\": doc[\"choices\"][\"text\"],\n",
    "            \"gold\": [\"A\", \"B\", \"C\", \"D\", \"E\"].index(doc[\"answerKey\"]),\n",
    "        }\n",
    "        return out_doc\n",
    "\n",
    "    # Process the document and create the full prompt\n",
    "    doc = _process_doc(doc)\n",
    "    prompt = doc[\"query\"]\n",
    "    answer = doc[\"choices\"][doc[\"gold\"]]\n",
    "    doc[\"full_prompt\"] = prompt + \" \" + answer\n",
    "    return doc\n",
    "\n",
    "# Load the ARC-Easy dataset\n",
    "arc_e_dataset = load_dataset(\"ai2_arc\", \"ARC-Easy\", split=\"train\")\n",
    "\n",
    "# Apply the add_prompt_func_arc function to each document in the dataset\n",
    "dataset = arc_e_dataset.map(add_prompt_func_arc)\n",
    "\n",
    "print(f\"Number of examples in the dataset: {len(dataset)}\")\n",
    "print(f\"Fields in the dataset: {list(dataset.features.keys())}\")"
   ],
   "metadata": {
    "id": "MElnIc345_pi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "üìù Tokenizing for Model Training:"
   ],
   "metadata": {
    "id": "OLx1ygLR_7wc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "    \"\"\"\n",
    "    Tokenizes the given prompt and optionally adds an end-of-sequence (EOS) token.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input text to tokenize.\n",
    "        add_eos_token (bool): Whether to add an EOS token at the end of the tokenized input.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing tokenized input ids, attention mask, and labels.\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt with truncation and padding\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=True,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # Add EOS token if necessary\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < 256\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    # Create labels for the tokenized input\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    \"\"\"\n",
    "    Generates a tokenized prompt from the data point.\n",
    "\n",
    "    Args:\n",
    "        data_point (dict): A dictionary containing the full prompt.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing tokenized input ids, attention mask, and labels.\n",
    "    \"\"\"\n",
    "    full_prompt = data_point[\"full_prompt\"]\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "# Shuffle the dataset and apply the generate_and_tokenize_prompt function to each data point\n",
    "dataset = dataset.shuffle().map(generate_and_tokenize_prompt)\n",
    "\n",
    "# Print the first example\n",
    "print(dataset[0])"
   ],
   "metadata": {
    "id": "TeYc6BpU6-XO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 8: Finetune the Model üéØ\n",
    "\n",
    "It's time to finetune our OpenELM model! We'll set up the training parameters, including batch size, learning rate, and evaluation strategy. üìä\n",
    "\n",
    "By feeding the model question-answer pairs from the `ARC-Easy` dataset, we can train it to generate more accurate and relevant responses. It enables the model to learn the unique patterns and relationships within the diverse topics covered by the dataset. üéØ"
   ],
   "metadata": {
    "id": "tm5F9yaq5_Ad"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# Define the path where the fine-tuned adapter will be saved\n",
    "finetuned_super_adapter_path = \"sqft-sparsepeft-openelm-1_1b-50-arce-adapter\"\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    # learning rate is already set in NNCF configuration.\n",
    "    # learning_rate=1e-4,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    output_dir=finetuned_super_adapter_path,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=None,\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    "    compression_ctrl=compression_ctrl,  # NLS requires\n",
    ")\n",
    "results = trainer.train()\n",
    "metrics = results.metrics\n",
    "metrics[\"train_samples\"] = len(dataset)\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ],
   "metadata": {
    "id": "lFu4V02y7YwL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 9: Extract the heuristic sub-adapter üîß\n",
    "\n",
    "Due to the use of the NLS strategy, we trained a super LoRA adapter. To quickly evaluate its quality and performance, we will use a heuristic sub-adapter (\"intermediate\" adapter) as a reference. The following code demonstrates how to extract a heuristic sub-adapter. üìä"
   ],
   "metadata": {
    "id": "Wl5AgMrWoio6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from peft.utils import CONFIG_NAME, SAFETENSORS_WEIGHTS_NAME\n",
    "from safetensors.torch import load_file, save_file\n",
    "\n",
    "# Load the super adapter weights\n",
    "super_adapter_weights = load_file(os.path.join(finetuned_super_adapter_path, SAFETENSORS_WEIGHTS_NAME))\n",
    "num_adapters = sum(\"lora_A\" in key for key in super_adapter_weights)\n",
    "\n",
    "# Initialize heuristic adapter configuration\n",
    "heu_adapter_config = [rank_search_space[(len(rank_search_space) - 1) // 2]] * num_adapters\n",
    "\n",
    "# Function to save sub-adapter weights based on the given configuration\n",
    "def save_sub_adapter(config, save_dir):\n",
    "    sub_adapter_weights = super_adapter_weights.copy()\n",
    "    num_pruned_adapter = 0\n",
    "    for key in sub_adapter_weights:\n",
    "        if \"lora_A\" not in key:\n",
    "            continue\n",
    "        rank = config[num_pruned_adapter]\n",
    "        sub_adapter_weights[key] = super_adapter_weights[key][:rank].clone()\n",
    "        lora_b_key = key.replace(\"lora_A\", \"lora_B\")\n",
    "        sub_adapter_weights[lora_b_key] = super_adapter_weights[lora_b_key][:, :rank].clone()\n",
    "        num_pruned_adapter += 1\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_file(sub_adapter_weights, os.path.join(save_dir, SAFETENSORS_WEIGHTS_NAME))\n",
    "    config_path = os.path.join(finetuned_super_adapter_path, CONFIG_NAME)\n",
    "    os.system(f\"cp {config_path} {save_dir}\")\n",
    "\n",
    "# save the heuristic sub-adapter\n",
    "sub_adapter_path = os.path.join(finetuned_super_adapter_path, \"heuristic_sub_adapter\")\n",
    "save_sub_adapter(heu_adapter_config, sub_adapter_path)"
   ],
   "metadata": {
    "id": "55LMdNWWok7_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 10: Search an optimal sub-adapter (Optional) üîç\n",
    "\n",
    "To get a better performing sub-adapter, we can leverage the simple hill-climbing algorithm to extract an optimal sub-adapter from the trained super LoRA adapter. The following code demonstrates the process of identifying and saving the best-performing sub-adapter configuration based on the validation set. üìä\n",
    "\n",
    "Before exploring the super adapter, we need to define a custom task for the search phase that applies the validation set of the `ARC-Easy`.\n",
    "Specifically, we define the task for the validation set by modifying the existing `arc_easy.yaml` configuration file to create a new task configuration for `arc_easy_val`.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "GGHF8QTb1jKY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "\n",
    "from lm_eval.utils import load_yaml_config\n",
    "\n",
    "# Get the directory path where the `lm_eval` module is located\n",
    "spec = importlib.util.find_spec(\"lm_eval\")\n",
    "module_path = spec.origin\n",
    "module_dir = os.path.dirname(module_path)\n",
    "arc_yaml_file = os.path.join(module_dir, \"tasks/arc/arc_easy.yaml\")\n",
    "task_config = load_yaml_config(arc_yaml_file)\n",
    "\n",
    "# Modify the task configuration to define the validation set task\n",
    "task_config[\"task\"] = \"arc_easy_val\"\n",
    "task_config[\"dataset_name\"] = \"ARC-Easy\"\n",
    "task_config[\"test_split\"] = \"validation\""
   ],
   "metadata": {
    "id": "PBnpqIJeTM_F"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we implement the hill-climbing algorithm to search for the optimal sub-adapter configuration. This involves evaluating different configurations on the validation set and selecting the best-performing one."
   ],
   "metadata": {
    "id": "yJbcqX830170"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import tempfile\n",
    "import yaml\n",
    "import importlib.util\n",
    "\n",
    "from lm_eval import evaluator\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "from lm_eval.tasks import TaskManager\n",
    "from lm_eval.utils import load_yaml_config\n",
    "from peft import PeftModel\n",
    "\n",
    "lm = HFLM(model, batch_size=64, trust_remote_code=True, add_bos_token=True)\n",
    "task_manager = TaskManager(\"INFO\", include_path=None)\n",
    "request_caching_args = {'cache_requests': False, 'delete_requests_cache': False, 'rewrite_requests_cache': False}\n",
    "\n",
    "# Hill-climbing algorithm\n",
    "t, T = 0, 10\n",
    "anchor_adapter_config = heu_adapter_config\n",
    "visited = set()\n",
    "\n",
    "while t < T:\n",
    "    # Find all possible sub-adapters in this turn\n",
    "    all_neighbors = []\n",
    "    for idx in range(len(anchor_adapter_config)):\n",
    "        cur_rank = anchor_adapter_config[idx]\n",
    "        space_idx = rank_search_space.index(cur_rank)\n",
    "        if space_idx != 0:\n",
    "            new_sub_adapter_config = anchor_adapter_config.copy()\n",
    "            new_sub_adapter_config[idx] = rank_search_space[space_idx - 1]\n",
    "            all_neighbors.append(new_sub_adapter_config)\n",
    "        if space_idx != len(rank_search_space) - 1:\n",
    "            new_sub_adapter_config = anchor_adapter_config.copy()\n",
    "            new_sub_adapter_config[idx] = rank_search_space[space_idx + 1]\n",
    "            all_neighbors.append(new_sub_adapter_config)\n",
    "    all_neighbors = [neighbor for neighbor in all_neighbors if str(neighbor) not in visited]\n",
    "    print(f\"Found {len(all_neighbors)} neighbors in Turn {t}\")\n",
    "\n",
    "    best = (None, -1.0)\n",
    "    for i, possible_config in enumerate(all_neighbors):\n",
    "        visited.add(str(possible_config))\n",
    "        with tempfile.TemporaryDirectory() as temp_sub_adapter_path:\n",
    "            save_sub_adapter(\n",
    "                possible_config,\n",
    "                temp_sub_adapter_path\n",
    "            )\n",
    "            result_log = os.path.join(temp_sub_adapter_path, \"result.json\")\n",
    "            model = PeftModel.from_pretrained(lm.model.get_base_model(), temp_sub_adapter_path)\n",
    "            # Update the model in HFLM (there might be a better way here)\n",
    "            lm._model = model\n",
    "\n",
    "            # Evaluate the current sub-adapter configuration\n",
    "            results = evaluator.simple_evaluate(\n",
    "                model=lm,\n",
    "                tasks=[task_config],\n",
    "                batch_size=64,\n",
    "                log_samples=False,\n",
    "                task_manager=task_manager,\n",
    "                **request_caching_args,\n",
    "            )\n",
    "            accuracy = results[\"results\"][\"arc_easy_val\"][\"acc_norm,none\"]\n",
    "            print(f\"Accuracy: {accuracy} ({i+1}/{len(all_neighbors)} possible sub-adapter in Turn {t})\")\n",
    "\n",
    "            # Update the best configuration if current one is better\n",
    "            if accuracy > best[1]:\n",
    "                best = (possible_config, accuracy)\n",
    "    print(f\"Best accuarcy in Turn {t} is {best[1]}\")\n",
    "    anchor_adapter_config = best[0]\n",
    "    t += 1\n",
    "\n",
    "# save the optimal sub-adapter\n",
    "sub_adapter_path = os.path.join(finetuned_super_adapter_path, \"optimal_sub_adapter\")\n",
    "save_sub_adapter(best[0], sub_adapter_path)"
   ],
   "metadata": {
    "id": "SwHIj55k1q_R"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this code, we first load the super adapter weights and initialize the heuristic configuration. We then define a function to save the sub-adapter weights based on the current configuration. The hill-climbing algorithm iterates through possible configurations, evaluates them on the validation set, and updates the best configuration found. Finally, we save the optimal sub-adapter configuration.\n",
    "\n",
    "By following these steps, we can effectively identify and save the best-performing sub-adapter configuration from the trained super LoRA adapter."
   ],
   "metadata": {
    "id": "3WrJ7DNn86dE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 11: Merge the Model üß©\n",
    "\n",
    "After finetuning and extracting a sub-adapter, it's time to put our model to the test! But first, we need to merge the heuristic sub-adapter weights with the base model. This step is essential because the adapter weights contain the adaptations learned during finetuning. By merging these weights, we effectively integrate the newly acquired knowledge into the base model. üéõÔ∏è\n",
    "\n",
    "To merge the weights, we'll use the `merge_and_unload()` function from the PEFT library. This function seamlessly combines the heuristic sub-adapter weights with the base model's corresponding weights, resulting in a unified model that incorporates the finetuned knowledge. üîß\n",
    "\n",
    "Once the heuristic sub-adapter weights are merged, we'll save the finetuned model to preserve its state. This ensures that we can easily load and use the finetuned model for future tasks without needing to repeat the finetuning process. ‚ú®"
   ],
   "metadata": {
    "id": "6Ie8OvVbDWY-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4b981a9-e0f7-4f7d-bc59-a3f54f51a772"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_model_path = \"sqft-openelm-1_1b-50-base\"\n",
    "tuned_model_path = \"sqft-sparsepeft-openelm-1_1b-50-arce\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_path, trust_remote_code=True).to(device)\n",
    "model = PeftModel.from_pretrained(base_model, sub_adapter_path)\n",
    "\n",
    "# Merge the adapter weights into the base model and unload the adapter\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.train(False)\n",
    "base_model.save_pretrained(tuned_model_path, state_dict=merged_model.state_dict())\n",
    "\n",
    "# Load and save the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)\n",
    "tokenizer.save_pretrained(tuned_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 12: Evaluate the Finetuned Model üèÜ\n",
    "\n",
    "Now, let's compare the result with the pre-finetuned model to see the improvements. Prepare to be amazed by the power of finetuning! ü§©\n",
    "\n",
    "By merging the adapter weights, we've ensured that our model is equipped to handle real-world tasks with its newly acquired knowledge. So, let's put it to the test and see how it performs! üåü\n"
   ],
   "metadata": {
    "id": "J0k3JSSYE9N5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!lm_eval --model hf --model_args pretrained=sqft-sparsepeft-openelm-1_1b-50-arce,add_bos_token=True,trust_remote_code=True --tasks arc_easy --batch_size auto:4"
   ],
   "metadata": {
    "id": "RiYK1kq2FSMg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can get the following results:\n",
    "\n",
    "| Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
    "|--------|------:|------|-----:|--------|-----:|---|-----:|\n",
    "|arc_easy|      1|none  |     0|acc     |0.5972|¬±  |0.0101|\n",
    "|        |       |none  |     0|acc_norm|0.6166|¬±  |0.0100|\n"
   ],
   "metadata": {
    "id": "YB54k_WEEL_3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The evaluation results clearly demonstrate the significant improvements achieved through fine-tuning the sparsified OpenELM model on the `ARC-Easy` dataset. Initially, the out-of-the-box performance of the model yielded an accuracy (acc) of 0.4966 and a normalized accuracy (acc_norm) of 0.4415. After fine-tuning, the model's accuracy increased to **0.5972**, and the normalized accuracy rose to **0.6166**. üìà Notably, this notebook is intended to introduce the usage process of SQFT and simply demonstrate the effectiveness of SQFT. Achieving better fine-tuning results requires more experiments and extensive parameter exploration. More importantly, NLS can also obtain better-performing sub-adapters through simple searches. üí•\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "In summary, our approach demonstrates that low-cost model adaptation in low-precision sparse foundation models can significantly enhance performance while maintaining efficiency. This experiment underscores the potential of low-precision and sparsity-aware methods in making machine learning more accessible and scalable. üöÄüåü\n",
    "\n",
    "We encourage you to test our cost-effective and efficient method on your custom fine-tuning datasets! üéâüìä"
   ],
   "metadata": {
    "id": "boC43Y9vQyYv"
   }
  }
 ]
}
