diff --git a/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_builder.py b/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_builder.py
index bc6464b24..ca2666626 100644
--- a/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_builder.py
+++ b/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_builder.py
@@ -152,3 +152,16 @@ class ElasticityBuilder(PTCompressionAlgorithmBuilder):
 
         # No conflict resolving with the related config options, parameters are overridden by compression state
         self._available_elasticity_dims = list(map(ElasticityDim, available_elasticity_dims_state))
+
+    def _are_frozen_layers_allowed(self):
+        """
+        Check if frozen layers are allowed based on NNCF configuration.
+        If specified in NNCF configuration, frozen layers will be allowed.
+
+        :return: A tuple where the first element is a boolean indicating if frozen layers are allowed,
+                 and the second element is a string message explaining the reason.
+        """
+        frozen_layers_allowed = self.config.get("bootstrapNAS", {}).get("training", {}).get("frozen_layers_allowed", False)
+        if frozen_layers_allowed:
+            return True, "Frozen layers are allowed (`frozen_layers_allowed` is set to True in NNCF config)"
+        return super()._are_frozen_layers_allowed()
diff --git a/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_builder.py b/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_builder.py
index 92609327f..7a0555e3e 100644
--- a/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_builder.py
+++ b/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_builder.py
@@ -152,3 +152,16 @@ class ProgressiveShrinkingBuilder(PTCompressionAlgorithmBuilder):
         self._bn_adapt_params = state_without_name[self._state_names.BN_ADAPTATION_PARAMS]
         bn_adapt_algo_kwargs = get_bn_adapt_algo_kwargs(self.config, self._bn_adapt_params)
         self._bn_adaptation = BatchnormAdaptationAlgorithm(**bn_adapt_algo_kwargs) if bn_adapt_algo_kwargs else None
+
+    def _are_frozen_layers_allowed(self):
+        """
+        Check if frozen layers are allowed based on the algorithm configuration.
+        If specified in the algorithm configuration, frozen layers will be allowed.
+
+        :return: A tuple where the first element is a boolean indicating if frozen layers are allowed,
+                 and the second element is a string message explaining the reason.
+        """
+        frozen_layers_allowed = self._algo_config.get("frozen_layers_allowed", False)
+        if frozen_layers_allowed:
+            return True, "Frozen layers are allowed (`frozen_layers_allowed` is set to True in the algorithm config)"
+        return super()._are_frozen_layers_allowed()
diff --git a/nncf/torch/layer_utils.py b/nncf/torch/layer_utils.py
index fb7d7bed7..3b8fda98e 100644
--- a/nncf/torch/layer_utils.py
+++ b/nncf/torch/layer_utils.py
@@ -127,6 +127,25 @@ class _NNCFModuleMixin:
                 results = op_results
         return results
 
+    def get_proxy_module(self, *args):
+        """
+        Gets a proxy module with pre-operations applied.
+
+        Args:
+            *args: Arguments for the pre-operations.
+
+        Returns:
+            ProxyModule: The proxy module with pre-operations applied.
+        """
+        proxy_module = ProxyModule(self)
+        for op in self.pre_ops.values():
+            op_args = op(proxy_module, args)
+            if op_args is not None:
+                if not isinstance(op_args, tuple):
+                    op_args = tuple([op_args])
+                args = op_args
+        return proxy_module
+
 
 class CompressionParameter(nn.Parameter):
     """
