{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Low-cost Model Adaptation in Low-precision Sparse Foundation Models: **SQFT + SparsePEFT (LoRA)** üöÄ"
   ],
   "metadata": {
    "id": "Yt3cghlzcITV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Welcome to an exhilarating journey as we delve into the realm of fine-tuning for efficient large language models (LLMs)! üåü\n",
    "\n",
    "We'll be working with Apple's OpenELM model and optimizing it for a specific task with one of our SQFT pipelines: **SQFT + SparsePEFT (LoRA)**."
   ],
   "metadata": {
    "id": "7HJgBi6Fcc3z"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All of our SQFT notebooks:\n",
    "\n",
    "- **SQFT (LoRA)**: [link]()\n",
    "- **SQFT (NLS)**: [link]()\n",
    "- **SQFT + SparsePEFT (LoRA)**: [link]()\n",
    "- **SQFT + SparsePEFT (NLS)**: [link]()\n",
    "- **SQFT + QA-SparsePEFT (LoRA)**: [link]()\n",
    "- **SQFT + QA-SparsePEFT (NLS)**: [link]()"
   ],
   "metadata": {
    "id": "mPWRSU2emi5d"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Overview\n",
    "\n",
    "In this notebook, you will learn a practical and novel solution that generates efficient (**sparse** or **quantized**) models fine-tuned for downstream-specific tasks for real-world applications. The notebook introduces the solution of SQFT + SparsePEFT (LoRA), covering the following key points:\n",
    "\n",
    "1. Setup the Environment ‚öôÔ∏è\n",
    "2. Sparsification ‚úÇÔ∏è\n",
    "3. Load Model üöÄ\n",
    "4. Test the Model Before Tuning üß™\n",
    "5. Configure the LoRA Settings üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
    "6. Prepare the Dataset üìö\n",
    "7. Finetune the Model üéØ\n",
    "8. Merge the Model üß©\n",
    "9. Evaluate the Finetuned Model üèÜ\n",
    "\n",
    "This notebook illustrates how fine-tuning can significantly boost the performance of a sparse model across a diverse array of topics, enhancing its versatility and applicability to various domains. You will gain valuable insights into the process of developing a **task-specific** highly-efficient model capable of delivering accurate and relevant responses to a broad spectrum of questions."
   ],
   "metadata": {
    "id": "CUJYLcWKdIdF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Quick Start"
   ],
   "metadata": {
    "id": "ryxZOqMzrADr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 1: Setup the Environment ‚öôÔ∏è\n",
    "\n",
    "Let's start by setting up our environment! We'll install all the essential packages, including the Hugging Face `transformers` library, `peft` library, and a few additional tools. üì¶\n",
    "Please follow https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning/tree/main/SQFT#setup to set up the environment for SQFT."
   ],
   "metadata": {
    "id": "mR5_H528fBzc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "üëÄ Check whether GPU is available:"
   ],
   "metadata": {
    "id": "Rc5uPuwijnxA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU is available. Using GPU.\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ],
   "metadata": {
    "id": "mUdnX7Yfj9mZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "üîë Logging into Hugging Face Hub:"
   ],
   "metadata": {
    "id": "Y0SffLEy-q_G"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19af72a5-9b23-4fe5-8ca8-bf45c7bdf7dc"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 2: Sparsification ‚úÇÔ∏è\n",
    "\n",
    "Before fine-tuning, SQFT employs a simple but effective pruning approach [Wanda](https://arxiv.org/abs/2306.11695) to\n",
    "sparsify the language model, serving as the base model (frozen) for adapter training.\n",
    "Clone the Wanda repo and apply our patch:"
   ],
   "metadata": {
    "id": "qJAqPaMhkidX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/locuslab/wanda.git && cd wanda && git checkout 8e8fc87 && git apply --ignore-space-change --ignore-whitespace ../wanda-modifications-for-sqft-usage.patch"
   ],
   "metadata": {
    "id": "TT0mOOvflPWQ",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below is the command for unstructured sparsifying [apple/OpenELM-1_1B](https://huggingface.co/apple/OpenELM-1_1B)\n",
    "with Wanda, to achieve unstructured 50% sparsity. Please note that we retain the model in FP32 precision to maximize its performance, and the tokenizer used by the OpenELM model is the one from [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)."
   ],
   "metadata": {
    "id": "MJaDTrYHlwMZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!python wanda/main.py --model apple/OpenELM-1_1B --dtype auto --tokenizer meta-llama/Llama-2-7b-hf --prune_method wanda --sparsity_ratio 0.5 --sparsity_type unstructured --save wanda_out --save_model sqft-openelm-1_1b-50-base"
   ],
   "metadata": {
    "id": "nSAocqMkmF4L",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "- `--model`: The identifier for the model on the Hugging Face model hub or local path.\n",
    "- `--sparsity_ratio`: Specifies the percentage of weights to be pruned.\n",
    "- `--save_model`: Specifies the directory where the pruned language model will be stored.\n",
    "\n",
    "Further details can be referred to [Wanda](https://github.com/locuslab/wanda).\n",
    "Note that the sparsifying step can be replaced by other sparse algorithms.\n",
    "Feel free to try other pruning approaches for the base model before training. üòä"
   ],
   "metadata": {
    "id": "h8qV71jemkOf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 3: Load Model üöÄ\n",
    "\n",
    "Let's get started by loading the sparsified OpenELM model with Hugging Face's `AutoModelForCausalLM` class. We'll also bring in the corresponding tokenizer to handle our input data preprocessing. üõ†Ô∏è"
   ],
   "metadata": {
    "id": "3jg9ssIkqF2M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"sqft-openelm-1_1b-50-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Set padding side to the right to ensure proper attention masking during fine-tuning\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(device)\n",
    "# Disable caching mechanism to reduce memory usage during fine-tuning\n",
    "model.config.use_cache = False"
   ],
   "metadata": {
    "id": "RFZl1xcPqcEn",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 4: Test the Model Before Tuning üß™\n",
    "\n",
    "Before diving into fine-tuning, let's first evaluate the out-of-the-box performance of the **sparsified** OpenELM model. We will use [lm-evaluation-harness v0.4.2](https://github.com/EleutherAI/lm-evaluation-harness/tree/v0.4.2) to assess the model on the `ARC-Easy` dataset with the default configuration. In the subsequent fine-tuning steps, we will also use the training set of `ARC-Easy` to observe the performance changes before and after fine-tuning. Please note that this notebook is intended to demonstrate the usage of SQFT, so a small dataset is used here for demonstration."
   ],
   "metadata": {
    "id": "BcdCZU1XAa82"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!lm_eval --model hf --model_args pretrained=sqft-openelm-1_1b-50-base,add_bos_token=True,trust_remote_code=True --tasks arc_easy --batch_size auto:4"
   ],
   "metadata": {
    "id": "a38j4NRbAaql"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can get the following results:\n",
    "\n",
    "| Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
    "|--------|------:|------|-----:|--------|-----:|---|-----:|\n",
    "|arc_easy|      1|none  |     0|acc     |0.4966|¬±  |0.0103|\n",
    "|        |       |none  |     0|acc_norm|0.4415|¬±  |0.0102|"
   ],
   "metadata": {
    "id": "Sooc2g6R6Wjp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 5: Configure the LoRA Settings üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
    "\n",
    "To efficiently finetune our model, we'll leverage the LoRA (Low-Rank Adaptation) technique.\n",
    "LoRA enables us to tailor the model to our specific task by training only a small subset of additional parameters. This significantly reduces on training time and memory usage! ‚è∞\n",
    "\n",
    "We'll set up the LoRA configuration by specifying the low rank (r) and the target modules we aim to adapt. For the SQFT + SparsePEFT solution (which is what this notebook introduces), we need to set `sparse_adapter` to `True` in order to make the adapter sparse, so that the adapter can be merged into the base model without any loss of sparsity. üéØ"
   ],
   "metadata": {
    "id": "nNarvwDdrbXG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare the model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"qkv_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    sparse_adapter=True, # SparsePEFT\n",
    ")\n",
    "\n",
    "# Apply the LoRA configuration to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ],
   "metadata": {
    "id": "SycK3eAUrpHq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 6: Prepare the Dataset üìö\n",
    "\n",
    "We prepare a domain-specific dataset to finetune our sparsified model.\n",
    "As tested in step 4, to introduce our process more simply, we will utilize a small dataset, `ARC-Easy`. This dataset features authentic grade-school level, multiple-choice science questions, designed to foster research in advanced question-answering. By honing in on the question-answer pairs, we aim to adapt our model to deliver precise and relevant responses to a variety of inquiries. üîç\n"
   ],
   "metadata": {
    "id": "gNqfEq5I5jMJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "üîß Loading Data and Preparing Prompts:"
   ],
   "metadata": {
    "id": "uNC-T-BMEyzm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "\"\"\"\n",
    "This function is inspired by the implementation in `lm-evaluation-harness` library.\n",
    "It processes the ARC-Easy dataset to create prompts for question-answering tasks.\n",
    "\"\"\"\n",
    "def add_prompt_func_arc(doc):\n",
    "\n",
    "    def _process_doc(doc):\n",
    "        \"\"\"\n",
    "        Process a single document to convert numeric answer keys to letters and\n",
    "        format the document into a dictionary.\n",
    "        \"\"\"\n",
    "        # Map numeric answer keys to letters\n",
    "        num_to_letter = {\"1\": \"A\", \"2\": \"B\", \"3\": \"C\", \"4\": \"D\", \"5\": \"E\"}\n",
    "        doc[\"answerKey\"] = num_to_letter.get(doc[\"answerKey\"], doc[\"answerKey\"])\n",
    "\n",
    "        # Create a dictionary with necessary fields\n",
    "        out_doc = {\n",
    "            \"id\": doc[\"id\"],\n",
    "            \"query\": \"Question: \" + doc[\"question\"] + \"\\nAnswer:\",\n",
    "            \"choices\": doc[\"choices\"][\"text\"],\n",
    "            \"gold\": [\"A\", \"B\", \"C\", \"D\", \"E\"].index(doc[\"answerKey\"]),\n",
    "        }\n",
    "        return out_doc\n",
    "\n",
    "    # Process the document and create the full prompt\n",
    "    doc = _process_doc(doc)\n",
    "    prompt = doc[\"query\"]\n",
    "    answer = doc[\"choices\"][doc[\"gold\"]]\n",
    "    doc[\"full_prompt\"] = prompt + \" \" + answer\n",
    "    return doc\n",
    "\n",
    "# Load the ARC-Easy dataset\n",
    "arc_e_dataset = load_dataset(\"ai2_arc\", \"ARC-Easy\", split=\"train\")\n",
    "\n",
    "# Apply the add_prompt_func_arc function to each document in the dataset\n",
    "dataset = arc_e_dataset.map(add_prompt_func_arc)\n",
    "\n",
    "print(f\"Number of examples in the dataset: {len(dataset)}\")\n",
    "print(f\"Fields in the dataset: {list(dataset.features.keys())}\")"
   ],
   "metadata": {
    "id": "MElnIc345_pi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "üìù Tokenizing for Model Training:"
   ],
   "metadata": {
    "id": "OLx1ygLR_7wc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "    \"\"\"\n",
    "    Tokenizes the given prompt and optionally adds an end-of-sequence (EOS) token.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input text to tokenize.\n",
    "        add_eos_token (bool): Whether to add an EOS token at the end of the tokenized input.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing tokenized input ids, attention mask, and labels.\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt with truncation and padding\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=True,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # Add EOS token if necessary\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < 256\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    # Create labels for the tokenized input\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    \"\"\"\n",
    "    Generates a tokenized prompt from the data point.\n",
    "\n",
    "    Args:\n",
    "        data_point (dict): A dictionary containing the full prompt.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing tokenized input ids, attention mask, and labels.\n",
    "    \"\"\"\n",
    "    full_prompt = data_point[\"full_prompt\"]\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "# Shuffle the dataset and apply the generate_and_tokenize_prompt function to each data point\n",
    "dataset = dataset.shuffle().map(generate_and_tokenize_prompt)\n",
    "\n",
    "# Print the first example\n",
    "print(dataset[0])"
   ],
   "metadata": {
    "id": "TeYc6BpU6-XO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 7: Finetune the Model üéØ\n",
    "\n",
    "It's time to finetune our OpenELM model! We'll set up the training parameters, including batch size, learning rate, and evaluation strategy. üìä\n",
    "\n",
    "By feeding the model question-answer pairs from the `ARC-Easy` dataset, we can train it to generate more accurate and relevant responses. It enables the model to learn the unique patterns and relationships within the diverse topics covered by the dataset. üéØ"
   ],
   "metadata": {
    "id": "tm5F9yaq5_Ad"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# Define the path where the fine-tuned adapter will be saved\n",
    "finetuned_adapter_path = \"sqft-sparsepeft-openelm-1_1b-50-arce-adapter\"\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-4,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    output_dir=finetuned_adapter_path,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=None,\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")\n",
    "results = trainer.train()\n",
    "metrics = results.metrics\n",
    "metrics[\"train_samples\"] = len(dataset)\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ],
   "metadata": {
    "id": "lFu4V02y7YwL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 8: Merge the Model üß©\n",
    "\n",
    "After finetuning, it's time to put our model to the test! But first, we need to merge the LoRA weights with the base model. This step is essential because the LoRA weights contain the adaptations learned during finetuning. By merging these weights, we effectively integrate the newly acquired knowledge into the base model. üéõÔ∏è\n",
    "\n",
    "To merge the LoRA weights, we'll use the `merge_and_unload()` function from the PEFT library. This function seamlessly combines the LoRA weights with the base model's corresponding weights, resulting in a unified model that incorporates the finetuned knowledge. üîß\n",
    "\n",
    "Once the LoRA weights are merged, we'll save the finetuned model to preserve its state. This ensures that we can easily load and use the finetuned model for future tasks without needing to repeat the finetuning process. ‚ú®"
   ],
   "metadata": {
    "id": "6Ie8OvVbDWY-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4b981a9-e0f7-4f7d-bc59-a3f54f51a772"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_model_path = \"sqft-openelm-1_1b-50-base\"\n",
    "tuned_model_path = \"sqft-sparsepeft-openelm-1_1b-50-arce\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_path, trust_remote_code=True).to(device)\n",
    "model = PeftModel.from_pretrained(base_model, finetuned_adapter_path)\n",
    "\n",
    "# Merge the adapter weights into the base model and unload the adapter\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.train(False)\n",
    "base_model.save_pretrained(tuned_model_path, state_dict=merged_model.state_dict())\n",
    "\n",
    "# Load and save the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)\n",
    "tokenizer.save_pretrained(tuned_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "#### Step 9: Evaluate the Finetuned Model üèÜ\n",
    "\n",
    "Now, let's compare the result with the pre-finetuned model to see the improvements. Prepare to be amazed by the power of finetuning! ü§©\n",
    "\n",
    "By merging the adapter weights, we've ensured that our model is equipped to handle real-world tasks with its newly acquired knowledge. So, let's put it to the test and see how it performs! üåü\n"
   ],
   "metadata": {
    "id": "J0k3JSSYE9N5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!lm_eval --model hf --model_args pretrained=sqft-sparsepeft-openelm-1_1b-50-arce,add_bos_token=True,trust_remote_code=True --tasks arc_easy --batch_size auto:4"
   ],
   "metadata": {
    "id": "RiYK1kq2FSMg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can get the following results:\n",
    "\n",
    "| Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
    "|--------|------:|------|-----:|--------|-----:|---|-----:|\n",
    "|arc_easy|      1|none  |     0|acc     |0.6065|¬±  |  0.01|\n",
    "|        |       |none  |     0|acc_norm|0.6128|¬±  |  0.01|\n",
    "\n"
   ],
   "metadata": {
    "id": "YB54k_WEEL_3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The evaluation results clearly demonstrate the significant improvements achieved through fine-tuning the sparsified OpenELM model on the `ARC-Easy` dataset. Initially, the out-of-the-box performance of the model yielded an accuracy (acc) of 0.4966 and a normalized accuracy (acc_norm) of 0.4415. After fine-tuning, the model's accuracy increased to **0.6065**, and the normalized accuracy rose to **0.6128**. üìà Notably, this notebook is intended to introduce the usage process of SQFT and simply demonstrate the effectiveness of SQFT. Achieving better fine-tuning results requires more experiments and extensive parameter exploration.\n",
    "\n",
    "### Summary\n",
    "\n",
    "In summary, our approach demonstrates that low-cost model adaptation in low-precision sparse foundation models can significantly enhance performance while maintaining efficiency. This experiment underscores the potential of low-precision and sparsity-aware methods in making machine learning more accessible and scalable. üöÄüåü\n",
    "\n",
    "We encourage you to test our cost-effective and efficient method on your custom fine-tuning datasets! üéâüìä"
   ],
   "metadata": {
    "id": "boC43Y9vQyYv"
   }
  }
 ]
}
