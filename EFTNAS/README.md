# EFTNAS

## Env Setup

### 1. NNCF
```
git clone https://github.com/openvinotoolkit/nncf.git nncf_eftnas
cd nncf_eftnas
git checkout 415c3c4d
# Apply Patch 
git apply path/to/nncf.patch
pip install -e .
```

### 2. Hugging Face - Transformers
```
git clone https://github.com/huggingface/transformers.git
cd transformers
git checkout v4.29.1
git apply /path/to/transformers.patch
pip install -e .
pip install -r examples/pytorch/text-classification/requirements.txt
```

## EFTNAS Results
eftnas_results
- BERT: {Subnet version}_{task}
    - Subnetwork version: S1 for EFTNAS-S1, S2 for EFTNAS-S2
    - task: datasets in GLUE benchmark, SQuADv1, SQuADv2
- ViT: ViT

## EFTNAS Configs
Configs are in the folder `eftnas_configs`.
- BERT: {Subnet version}_{task}.json
    - Subnetwork version: S1 for EFTNAS-S1, S2 for EFTNAS-S2
    - task: datasets in GLUE benchmark, SQuADv1, SQuADv2
- ViT: ViT.json

### Config Explanation (Extra parameters in the nncf config)
- **movement_sparsity_total_epochs**: Used for movement sparsity. This overwrites `num_train_epochs` in the following scripts. It is similar to `epoch_lr` in BootstrapNAS. The real training epochs depends on `warmup_end_epoch` in movement_sparsity params.
- **model_name_or_path**: Overwrite `model_name_or_path` in the following scripts.
- **kd_teacher_model**: Teacher model.
- **reorg_cache_model**: Cache model of trained importance weight.

## Reproduce Results

** In the following scripts, nncf_config.json will overwrite some hyper-parameters.
For example, `model_name_or_path` and `num_train_epochs`.

** Besides EFTNAS-S2 + BERT + GLUE benchmark, other fine-tuned supernets are from HuggingFace Hub.

### Search Space Generation

#### Step1. Train an importance weight
```
cd /path/to/nncf
./eftnas_search_space/generate_importance_weight.sh DEVICE TRANSFORMER_PATH /path/to/nncf/eftnas_configs/eftnas_search_space_demo.json OUT_DIR
```
- DEVICE: gpu device
- TRANSFORMER_PATH: path to transformers installed via [2-transformers](#2-transformers)
- OUT_DIR: output dir

#### Step2. Generate search space
```
cd /path/to/nncf
python eftnas_search_space/generate_eftnas_search_space.py --importance_weight_dir OUT_DIR --config_save_name SAVE_NAME
```
- OUT_DIR: output dir used in step1
- SAVE_NAME: name of the saved nncf config. This saved nncf config includes the search space generated by eftnas.

### Train BERT - GLUE

#### Train BERT - get finetuned supernet (EFTNAS-S2)
```
cd /path/to/transformers
CUDA_VISIBLE_DEVICES=${DEVICES}  python examples/pytorch/text-classification/run_glue.py \
    --model_name_or_path google/bert_uncased_L-8_H-512_A-8 \
    --task_name $TASK_NAME \
    --output_dir ${OUT_DIR} \
    --overwrite_output_dir \
    --do_train \
    --do_eval \
    --do_search \
    --max_seq_length 128 \
    --per_device_train_batch_size 32 \
    --per_device_eval_batch_size 32 \
    --learning_rate 2e-5 \
    --num_train_epochs ${TRAINED_EPOCHS} \
    --evaluation_strategy epoch \
    --save_strategy epoch \
    --save_total_limit 1 \
    --fp16 \
    --seed 42
```
- TASK_NAME: mnli, qqp, qnli, cola, rte, sst2, mrpc
- OUT_DIR: Your output dir
- TRAINED_EPOCHS:
    - 5: cola, mrpc, qnli, rte, sst2
    - 3: mnli, qqp

#### Train BERT - NAS part
```
cd /path/to/transformers
CUDA_VISIBLE_DEVICES=${DEVICES}  python examples/pytorch/text-classification/run_glue.py \
    --model_name_or_path bert-base-uncased \
    --task_name $TASK_NAME \
    --nncf_config ${CONFIG} \
    --output_dir ${OUT_DIR} \
    --overwrite_output_dir \
    --do_train \
    --do_eval \
    --do_search \
    --max_seq_length 128 \
    --per_device_train_batch_size 32 \
    --per_device_eval_batch_size 128 \
    --learning_rate 2e-5 \
    --num_train_epochs 5 \
    --evaluation_strategy epoch \
    --save_strategy epoch \
    --save_total_limit 1 \
    --seed 42 \
    --fp16
```
- TASK_NAME: mnli, qqp, qnli, cola, rte, sst2, mrpc
- CONFIG: Select one from `eftnas_configs`.
- OUT_DIR: Your output dir

### Train BERT - SQuADv1.1

```
cd /path/to/transformers
CUDA_VISIBLE_DEVICES=${DEVICES} python examples/pytorch/question-answering/run_qa.py \
    --model_name_or_path bert-base-uncased \
    --do_train \
    --do_eval \
    --do_search \
    --dataset_name squad \
    --learning_rate 3e-5 \
    --per_gpu_train_batch_size 16 \
    --per_gpu_eval_batch_size 128 \
    --output_dir ${OUT_DIR} \
    --max_seq_length 384 \
    --doc_stride 128 \
    --nncf_config ${CONFIG} \
    --evaluation_strategy epoch \
    --save_strategy epoch \
    --metric_for_best_model f1 \
    --overwrite_output_dir \
    --save_total_limit 1 \
    --num_train_epochs 8 \
    --fp16
```
- CONFIG: Select one from `eftnas_configs`.
- OUT_DIR: Your output dir

### Train BERT - SQuADv2.0

```
CUDA_VISIBLE_DEVICES=${DEVICES} python examples/pytorch/question-answering/run_qa.py \
    --model_name_or_path bert-base-uncased \
    --do_train \
    --do_eval \
    --do_search \
    --dataset_name squad_v2 \
    --learning_rate 3e-5 \
    --per_gpu_train_batch_size 16 \
    --per_gpu_eval_batch_size 128 \
    --version_2_with_negative \
    --output_dir ${OUT_DIR} \
    --max_seq_length 384 \
    --doc_stride 128 \
    --nncf_config ${CONFIG} \
    --evaluation_strategy epoch \
    --save_strategy epoch \
    --metric_for_best_model f1 \
    --kd_teacher_model deepset/bert-base-uncased-squad2 \
    --ddp_find_unused_parameters True \
    --overwrite_output_dir \
    --save_total_limit 1 \
    --num_train_epochs 8 \
    --fp16
```
- CONFIG: Select one from `eftnas_configs`.
- OUT_DIR: Your output dir

### Train ViT

#### Train ViT - importance mask

```
cd /path/to/nncf/examples/experimental/torch/classification/
CUDA_VISIBLE_DEVICES=${DEVICES} python get_importance_mask.py \
    --mode train \
    --config eftnas_configs/ViT.json \
    --data ${IMAGENET_DIR} \
    --log-dir ${OUT_DIR}
```

- IMAGENET_DIR: path to imagenet dataset. 
- OUT_DIR: Your output dir.

#### Train ViT - NAS part

```
cd /path/to/nncf/examples/experimental/torch/classification/
CUDA_VISIBLE_DEVICES=${DEVICES} torchrun --nproc_per_node=2 --master_port ${MASTER_PORT} bootstrap_nas.py \
    --mode train \
    --config eftnas_configs/ViT.json \
    --data ${IMAGENET_DIR} \
    --log-dir $OUT_DIR \
    --importance-mask-weight ${IMPORTANCE_MASK_WEIGHT}
```

- IMAGENET_DIR: path to imagenet dataset. 
- MASTER_PORT: master port
- OUT_DIR: Your output dir.
- IMPORTANCE_MASK_WEIGHT: ../eftnas_vit/model_best.pth)


# Citation

If you use the code or data in your research, please use the following BibTex entry:

```
@inproceedings{
munoz2024eftnas,
title={Searching for Efficient Language Models in First-Order Weight-Reordered Super-Networks},
author={J. Pablo Munoz and Yi Zheng and Nilesh Jain},
booktitle={The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation},
year={2024},
url={}
}
```