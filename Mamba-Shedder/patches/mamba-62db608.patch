diff --git a/mamba_ssm/models/mixer_seq_simple.py b/mamba_ssm/models/mixer_seq_simple.py
index fae2257..1a9ce98 100644
--- a/mamba_ssm/models/mixer_seq_simple.py
+++ b/mamba_ssm/models/mixer_seq_simple.py
@@ -288,10 +288,50 @@ class MambaLMHeadModel(nn.Module, GenerationMixin):
         config_data = load_config_hf(pretrained_model_name)
         config = MambaConfig(**config_data)
         model = cls(config, device=device, dtype=dtype, **kwargs)
-        model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))
+        info = model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype), strict=False)
+        if len(info.missing_keys) > 0:
+            def get_layer_by_key(model, key, layer_depth=3):
+                # Split the key into parts
+                parts = key.split('.')
+        
+                # Only keep the parts up to the specified layer depth
+                layer_parts = parts[:layer_depth]
+        
+                # Traverse the model using the layer parts
+                module = model
+                for part in layer_parts:
+                    if hasattr(module, part):
+                        module = getattr(module, part)
+                    else:
+                        raise AttributeError(f"Module does not have attribute '{part}'")
+        
+                return module
+        
+            for key in info.missing_keys:
+                layer = get_layer_by_key(model, key)
+                if "backbone.layers" in key and (".mixer.D" in key or ".mixer.dt_bias" in key):
+                    # SSM Pruning (currently only supported on Mamba2)
+                    if hasattr(layer.mixer, "mask_ssm"):
+                        layer.mixer.mask_ssm = True
+                        print(
+                            f"Some weights of SSM block in {layer.__class__.__name__} were not initialized from the model checkpoint at"
+                            f" {pretrained_model_name} and the corresponding SSM is pruned: {key}"
+                        )
+                elif "backbone.layers" in key:
+                    # Mamba Block Pruning
+                    assert hasattr(layer, "mask_mamba_block")
+                    layer.mask_mamba_block = True
+                    layer.norm = None
+                    layer.mixer = None
+                    print(
+                        f"Some weights of Mamba block in {layer.__class__.__name__} were not initialized from the model checkpoint at"
+                        f" {pretrained_model_name} and the corresponding Mamba block is pruned: {key}"
+                    )
+            torch.cuda.empty_cache()
+
         return model
 
-    def save_pretrained(self, save_directory):
+    def save_pretrained(self, save_directory, state_dict=None):
         """
         Minimal implementation of save_pretrained for MambaLMHeadModel.
         Save the model and its configuration file to a directory.
@@ -301,7 +341,10 @@ class MambaLMHeadModel(nn.Module, GenerationMixin):
 
         # Save the model's state_dict
         model_path = os.path.join(save_directory, 'pytorch_model.bin')
-        torch.save(self.state_dict(), model_path)
+        if state_dict is not None:
+            torch.save(state_dict, model_path)
+        else:
+            torch.save(self.state_dict(), model_path)
 
         # Save the configuration of the model
         config_path = os.path.join(save_directory, 'config.json')
diff --git a/mamba_ssm/modules/block.py b/mamba_ssm/modules/block.py
index 1bd968a..4ebcb3e 100644
--- a/mamba_ssm/modules/block.py
+++ b/mamba_ssm/modules/block.py
@@ -38,6 +38,7 @@ class Block(nn.Module):
             assert isinstance(
                 self.norm, (nn.LayerNorm, RMSNorm)
             ), "Only LayerNorm and RMSNorm are supported for fused_add_norm"
+        self.mask_mamba_block = False
 
     def forward(
             self, hidden_states: Tensor, residual: Optional[Tensor] = None, inference_params=None, **mixer_kwargs
@@ -48,6 +49,9 @@ class Block(nn.Module):
             hidden_states: the sequence to the encoder layer (required).
             residual: hidden_states = Mixer(LN(residual))
         """
+        if self.mask_mamba_block:
+            return hidden_states, residual
+
         if not self.fused_add_norm:
             residual = (hidden_states + residual) if residual is not None else hidden_states
             hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))
diff --git a/mamba_ssm/modules/mamba2.py b/mamba_ssm/modules/mamba2.py
index 1859ab0..024b088 100644
--- a/mamba_ssm/modules/mamba2.py
+++ b/mamba_ssm/modules/mamba2.py
@@ -150,6 +150,7 @@ class Mamba2(nn.Module, PyTorchModelHubMixin):
             self.out_proj = RowParallelLinear(self.d_inner * self.world_size, self.d_model, bias=bias,
                                               process_group=self.process_group, sequence_parallel=self.sequence_parallel,
                                               **factory_kwargs)
+        self.mask_ssm = False
 
     def forward(self, u, seqlen=None, seq_idx=None, cu_seqlens=None, inference_params=None):
         """
@@ -181,7 +182,7 @@ class Mamba2(nn.Module, PyTorchModelHubMixin):
         # If the model is loaded in fp16, without the .float() here, A might be -inf
         A = -torch.exp(self.A_log.float())  # (nheads) or (d_inner, d_state)
         dt_limit_kwargs = {} if self.dt_limit == (0.0, float("inf")) else dict(dt_limit=self.dt_limit)
-        if self.use_mem_eff_path and inference_params is None:
+        if self.use_mem_eff_path and inference_params is None and not self.mask_ssm:
             out = mamba_split_conv1d_scan_combined(
                 zxbcdt,
                 rearrange(self.conv1d.weight, "d 1 w -> d w"),
@@ -241,31 +242,34 @@ class Mamba2(nn.Module, PyTorchModelHubMixin):
                     seq_idx=seq_idx,
                 ).transpose(1, 2)
             x, B, C = torch.split(xBC, [self.d_ssm, self.ngroups * self.d_state, self.ngroups * self.d_state], dim=-1)
-            y = mamba_chunk_scan_combined(
-                rearrange(x, "b l (h p) -> b l h p", p=self.headdim),
-                dt,
-                A,
-                rearrange(B, "b l (g n) -> b l g n", g=self.ngroups),
-                rearrange(C, "b l (g n) -> b l g n", g=self.ngroups),
-                chunk_size=self.chunk_size,
-                D=rearrange(self.D, "(h p) -> h p", p=self.headdim) if self.D_has_hdim else self.D,
-                z=rearrange(z, "b l (h p) -> b l h p", p=self.headdim) if not self.rmsnorm else None,
-                dt_bias=self.dt_bias,
-                dt_softplus=True,
-                seq_idx=seq_idx,
-                cu_seqlens=cu_seqlens,
-                **dt_limit_kwargs,
-                return_final_states=ssm_state is not None,
-                return_varlen_states=cu_seqlens is not None and inference_params is not None,
-            )
-            if ssm_state is not None:
-                y, last_state, *rest = y
-                if cu_seqlens is None:
-                    ssm_state.copy_(last_state)
-                else:
-                    varlen_states = rest[0]
-                    ssm_state.copy_(varlen_states)
-            y = rearrange(y, "b l h p -> b l (h p)")
+            if self.mask_ssm:
+                y = x if self.rmsnorm else x * self.act(z)
+            else:
+                y = mamba_chunk_scan_combined(
+                    rearrange(x, "b l (h p) -> b l h p", p=self.headdim),
+                    dt,
+                    A,
+                    rearrange(B, "b l (g n) -> b l g n", g=self.ngroups),
+                    rearrange(C, "b l (g n) -> b l g n", g=self.ngroups),
+                    chunk_size=self.chunk_size,
+                    D=rearrange(self.D, "(h p) -> h p", p=self.headdim) if self.D_has_hdim else self.D,
+                    z=rearrange(z, "b l (h p) -> b l h p", p=self.headdim) if not self.rmsnorm else None,
+                    dt_bias=self.dt_bias,
+                    dt_softplus=True,
+                    seq_idx=seq_idx,
+                    cu_seqlens=cu_seqlens,
+                    **dt_limit_kwargs,
+                    return_final_states=ssm_state is not None,
+                    return_varlen_states=cu_seqlens is not None and inference_params is not None,
+                )
+                if ssm_state is not None:
+                    y, last_state, *rest = y
+                    if cu_seqlens is None:
+                        ssm_state.copy_(last_state)
+                    else:
+                        varlen_states = rest[0]
+                        ssm_state.copy_(varlen_states)
+                y = rearrange(y, "b l h p -> b l (h p)")
             if self.rmsnorm:
                 y = self.norm(y, z)
             if d_mlp > 0:
@@ -307,7 +311,9 @@ class Mamba2(nn.Module, PyTorchModelHubMixin):
         A = -torch.exp(self.A_log.float())  # (nheads,)
 
         # SSM step
-        if selective_state_update is None:
+        if self.mask_ssm:
+            y = x if self.rmsnorm else x * self.act(z)
+        elif selective_state_update is None:
             assert self.ngroups == 1, "Only support ngroups=1 for this inference code path"
             # Discretize A and B
             dt = F.softplus(dt + self.dt_bias.to(dtype=dt.dtype))  # (batch, nheads)
diff --git a/mamba_ssm/modules/mamba_simple.py b/mamba_ssm/modules/mamba_simple.py
index 4c8a388..dde0322 100644
--- a/mamba_ssm/modules/mamba_simple.py
+++ b/mamba_ssm/modules/mamba_simple.py
@@ -115,6 +115,7 @@ class Mamba(nn.Module):
         self.D._no_weight_decay = True
 
         self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)
+        self.mask_ssm = False
 
     def forward(self, hidden_states, inference_params=None):
         """
@@ -142,7 +143,7 @@ class Mamba(nn.Module):
 
         A = -torch.exp(self.A_log.float())  # (d_inner, d_state)
         # In the backward pass we write dx and dz next to each other to avoid torch.cat
-        if self.use_fast_path and causal_conv1d_fn is not None and inference_params is None:  # Doesn't support outputting the states
+        if self.use_fast_path and causal_conv1d_fn is not None and inference_params is None and not self.mask_ssm:  # Doesn't support outputting the states
             out = mamba_inner_fn(
                 xz,
                 self.conv1d.weight,
@@ -186,21 +187,25 @@ class Mamba(nn.Module):
             B = rearrange(B, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
             C = rearrange(C, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
             assert self.activation in ["silu", "swish"]
-            y = selective_scan_fn(
-                x,
-                dt,
-                A,
-                B,
-                C,
-                self.D.float(),
-                z=z,
-                delta_bias=self.dt_proj.bias.float(),
-                delta_softplus=True,
-                return_last_state=ssm_state is not None,
-            )
+            if self.mask_ssm:
+                y = x
+            else:
+                y = selective_scan_fn(
+                    x,
+                    dt,
+                    A,
+                    B,
+                    C,
+                    self.D.float(),
+                    # z=z,
+                    delta_bias=self.dt_proj.bias.float(),
+                    delta_softplus=True,
+                    return_last_state=ssm_state is not None,
+                )
             if ssm_state is not None:
                 y, last_state = y
                 ssm_state.copy_(last_state)
+            y = y * self.act(z)
             y = rearrange(y, "b d l -> b l d")
             out = self.out_proj(y)
         return out
@@ -235,7 +240,9 @@ class Mamba(nn.Module):
         A = -torch.exp(self.A_log.float())  # (d_inner, d_state)
 
         # SSM step
-        if selective_state_update is None:
+        if self.mask_ssm:
+            y = x * self.act(z)
+        elif selective_state_update is None:
             # Discretize A and B
             dt = F.softplus(dt + self.dt_proj.bias.to(dtype=dt.dtype))
             dA = torch.exp(torch.einsum("bd,dn->bdn", dt, A))
