diff --git a/src/transformers/modeling_utils.py b/src/transformers/modeling_utils.py
index e831ba361..d48a43a04 100755
--- a/src/transformers/modeling_utils.py
+++ b/src/transformers/modeling_utils.py
@@ -3862,6 +3862,58 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
                 keep_in_fp32_modules=keep_in_fp32_modules,
                 gguf_path=gguf_path,
             )
+      
+        def get_layer_by_key(model, key, layer_depth=3):
+            # Split the key into parts
+            parts = key.split('.')
+        
+            # Only keep the parts up to the specified layer depth
+            layer_parts = parts[:layer_depth]
+        
+            # Traverse the model using the layer parts
+            module = model
+            for part in layer_parts:
+                if hasattr(module, part):
+                    module = getattr(module, part)
+                else:
+                    raise AttributeError(f"Module does not have attribute '{part}'")
+        
+            return module
+        
+        if len(missing_keys) > 0:
+            # for compressed zamba2 from Mamba-Shedder
+            for key in missing_keys:
+                layer = get_layer_by_key(model, key)
+                if ".self_attn" in key:
+                    layer.mask_self_attn = True
+                    layer.self_attn = None
+                    logger.warning(
+                        f"Some weights of MHA module in {layer.__class__.__name__} were not initialized from the model checkpoint at"
+                        f" {pretrained_model_name_or_path} and the corresponding MHA module is pruned: {key}"
+                    )
+                elif ".feed_forward" in key:
+                    layer.mask_feed_forward = True
+                    layer.feed_forward = None
+                    logger.warning(
+                        f"Some weights of MLP module in {layer.__class__.__name__} were not initialized from the model checkpoint at"
+                        f" {pretrained_model_name_or_path} and the corresponding MLP module is pruned: {key}"
+                    )
+                elif ".mamba_layers" in key and (".mamba.D" in key or ".mamba.dt_bias" in key):
+                    # ssm
+                    layer.mamba.mask_ssm = True
+                    logger.warning(
+                        f"Some weights of SSM block in {layer.__class__.__name__} were not initialized from the model checkpoint at"
+                        f" {pretrained_model_name_or_path} and the corresponding SSM is pruned: {key}"
+                    )
+                elif ".mamba_layers" in key:
+                    layer.mask_mamba_block = True
+                    layer.mamba = None
+                    logger.warning(
+                        f"Some weights of Mamba block in {layer.__class__.__name__} were not initialized from the model checkpoint at"
+                        f" {pretrained_model_name_or_path} and the corresponding Mamba block is pruned: {key}"
+                    )
+            torch.cuda.empty_cache()
+
 
         # make sure token embedding weights are still tied if needed
         model.tie_weights()
@@ -4219,6 +4271,34 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
             }
         else:
             offload_index = None
+        
+        def module_reshape(state_dict):
+            for param_name, param in state_dict.items():
+                if ".bias" in param_name:
+                    continue
+                tensor_name = param_name
+                splits = tensor_name.split(".")
+                if len(splits) > 1:
+                    module = model_to_load
+                    parent = None
+                    for split in splits[:-1]:
+                        new_module = getattr(module, split)
+                        if new_module is None:
+                            raise ValueError(f"{module} has no attribute {split}.")
+                        parent = module
+                        module = new_module
+                    tensor_name = splits[-1]
+                    old_value = getattr(module, tensor_name)
+                    if old_value.shape != param.shape and isinstance(module, nn.Linear):
+                        new_module = torch.nn.Linear(
+                            param.shape[1],
+                            param.shape[0],
+                            bias=module.bias is not None,
+                            dtype=module.weight.dtype,
+                            device=module.weight.device
+                        )
+                        setattr(parent, splits[-2], new_module)
+
 
         if state_dict is not None:
             # Whole checkpoint
@@ -4230,6 +4310,7 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
                 remove_prefix_from_model,
                 ignore_mismatched_sizes,
             )
+            module_reshape(state_dict)
 
             # For GGUF models `state_dict` is never set to None as the state dict is always small
             if gguf_path:
@@ -4296,6 +4377,7 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
                     remove_prefix_from_model,
                     ignore_mismatched_sizes,
                 )
+                module_reshape(state_dict)
                 if low_cpu_mem_usage:
                     if is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized:
                         for key, param in model_to_load.state_dict().items():
diff --git a/src/transformers/models/zamba2/mamba2_layer.py b/src/transformers/models/zamba2/mamba2_layer.py
index a3f99430f..7ddb61658 100755
--- a/src/transformers/models/zamba2/mamba2_layer.py
+++ b/src/transformers/models/zamba2/mamba2_layer.py
@@ -116,7 +116,7 @@ class Mamba2Layer(nn.Module):
 
         assert A_init_range[0] > 0 and A_init_range[1] >= A_init_range[0]
         A = torch.empty(self.nheads, dtype=torch.float32).uniform_(*A_init_range)
-        A_log = torch.log(A).to(dtype=torch.bfloat16)
+        A_log = torch.log(A).to(dtype=torch.float32)
         self.A_log = nn.Parameter(A_log)
         self.A_log._no_weight_decay = True
 
@@ -141,6 +141,8 @@ class Mamba2Layer(nn.Module):
             self.out_proj_lora_A = nn.Linear(self.d_inner, self.config.lora_rank, bias = False)
             self.out_proj_lora_B = nn.Linear(self.config.lora_rank, self.d_model, bias = False)
             nn.init.zeros_(self.out_proj_lora_B.weight)
+        
+        self.mask_ssm = False
 
 
     def forward(self, 
@@ -183,7 +185,7 @@ class Mamba2Layer(nn.Module):
         input_not_masked = True
         if attention_mask is not None:
             input_not_masked = torch.all(attention_mask==1)
-        if self.use_mem_eff_path and inference_params is None and input_not_masked and not self.config.ft_lora:
+        if self.use_mem_eff_path and inference_params is None and input_not_masked and not self.config.ft_lora and not self.mask_ssm:
             out = mamba_split_conv1d_scan_combined(
                 zxbcdt,
                 rearrange(self.conv1d.weight, "d 1 w -> d w"),
@@ -241,25 +243,28 @@ class Mamba2Layer(nn.Module):
                 lora_output_z = self.z_lora_B(lora_output_z)
                 x = x + lora_output_x
                 z = z + lora_output_z
-            y = mamba_chunk_scan_combined(
-                rearrange(x, "b l (h p) -> b l h p", p=self.headdim),
-                dt,
-                A,
-                rearrange(B, "b l (g n) -> b l g n", g=self.ngroups),
-                rearrange(C, "b l (g n) -> b l g n", g=self.ngroups),
-                chunk_size=self.chunk_size,
-                D=rearrange(self.D, "(h p) -> h p", p=self.headdim) if self.D_has_hdim else self.D,
-                z=rearrange(z, "b l (h p) -> b l h p", p=self.headdim) if not self.rmsnorm else None,
-                dt_bias=self.dt_bias,
-                dt_softplus=True,
-                seq_idx=seq_idx,
-                **dt_limit_kwargs,
-                return_final_states=ssm_state is not None,
-            )
-            if ssm_state is not None:
-                y, last_state = y
-                ssm_state.copy_(last_state)
-            y = rearrange(y, "b l h p -> b l (h p)")
+            if self.mask_ssm:
+                y = x if self.rmsnorm else x * self.act(z)
+            else:
+                y = mamba_chunk_scan_combined(
+                    rearrange(x, "b l (h p) -> b l h p", p=self.headdim),
+                    dt,
+                    A,
+                    rearrange(B, "b l (g n) -> b l g n", g=self.ngroups),
+                    rearrange(C, "b l (g n) -> b l g n", g=self.ngroups),
+                    chunk_size=self.chunk_size,
+                    D=rearrange(self.D, "(h p) -> h p", p=self.headdim) if self.D_has_hdim else self.D,
+                    z=rearrange(z, "b l (h p) -> b l h p", p=self.headdim) if not self.rmsnorm else None,
+                    dt_bias=self.dt_bias,
+                    dt_softplus=True,
+                    seq_idx=seq_idx,
+                    **dt_limit_kwargs,
+                    return_final_states=ssm_state is not None,
+                )
+                if ssm_state is not None:
+                    y, last_state = y
+                    ssm_state.copy_(last_state)
+                y = rearrange(y, "b l h p -> b l (h p)")
             if self.rmsnorm:
                 y = self.norm(y, z)
             if d_mlp > 0:
@@ -314,7 +319,9 @@ class Mamba2Layer(nn.Module):
         A = -torch.exp(self.A_log.float())  # (nheads,)
 
         # SSM step
-        if selective_state_update is None:
+        if self.mask_ssm:
+            y = x if self.rmsnorm else x * self.act(z)
+        elif selective_state_update is None:
             assert self.ngroups == 1, "Only support ngroups=1 for this inference code path"
             # Discretize A and B
             dt = F.softplus(dt + self.dt_bias.to(dtype=dt.dtype))  # (batch, nheads)
@@ -384,6 +391,7 @@ class Mamba2Layer(nn.Module):
                 device=self.in_proj.weight.device,
                 dtype=torch.bfloat16
             )
+            ssm_state = ssm_state.to(self.in_proj[0].weight.dtype)
             inference_params.key_value_memory_dict_mamba[self.layer_idx] = (conv_state, ssm_state)
         else:
             conv_state, ssm_state = inference_params.key_value_memory_dict_mamba[self.layer_idx]
diff --git a/src/transformers/models/zamba2/modeling_zamba2.py b/src/transformers/models/zamba2/modeling_zamba2.py
index 546737604..511038324 100755
--- a/src/transformers/models/zamba2/modeling_zamba2.py
+++ b/src/transformers/models/zamba2/modeling_zamba2.py
@@ -834,50 +834,15 @@ class Zamba2MLP(nn.Module):
         self.layer = layer_idx
         ffn_hidden_size_1 = self.config.ffn_hidden_size
         ffn_hidden_size_2 = self.config.ffn_hidden_size
-        # If this is a gated linear unit we double the output width, see https://arxiv.org/pdf/2002.05202.pdf
-        if self.config.gated_linear_unit:
-            ffn_hidden_size_1 *= 2
-
-        if self.layer == -1:
-            ffn_hidden_size_1 = 8 * self.config.hidden_size
-
-        self.linear_fc1 = nn.Linear(self.config.hidden_size, ffn_hidden_size_1, bias = self.config.add_bias_linear)
-        if self.config.gated_linear_unit or self.layer == -1:
-
-            def glu(x):
-                x = torch.chunk(x, 2, dim=-1)
-
-                return F.gelu(x[0]) * x[1]
-            self.activation_func = glu
-        else:
-            self.activation_func = F.gelu
-
-
-        self.linear_fc2 = nn.Linear(ffn_hidden_size_2, self.config.hidden_size, bias = self.config.add_bias_linear)
         
-        if self.config.use_shared_block_lora:
-            self.linear_fc1_lora_A_list = nn.ParameterList([])
-            self.linear_fc1_lora_B_list = nn.ParameterList([])
-            for i in range(self.num_mem_blocks):
-                linear_fc1_lora_A = nn.Linear(self.config.hidden_size, self.config.lora_rank, bias = False)
-                linear_fc1_lora_B = nn.Linear(self.config.lora_rank, ffn_hidden_size_1, bias = False)
-                self.linear_fc1_lora_A_list.append(linear_fc1_lora_A)
-                self.linear_fc1_lora_B_list.append(linear_fc1_lora_B)
+        self.linear_fc1_up = nn.Linear(self.config.hidden_size, ffn_hidden_size_1, bias=self.config.add_bias_linear)
+        self.linear_fc1_gate = nn.Linear(self.config.hidden_size, ffn_hidden_size_1, bias=self.config.add_bias_linear)
+        self.linear_fc2 = nn.Linear(ffn_hidden_size_2, self.config.hidden_size, bias = self.config.add_bias_linear)
 
     def forward(self, hidden_states, inference_params=None, forward_layer_idx = None):
-
-        # [s, b, 4 * h/p]
-        if self.config.use_shared_block_lora:
-            linear_fc1_lora_A = self.linear_fc1_lora_A_list[forward_layer_idx]
-            linear_fc1_lora_B = self.linear_fc1_lora_B_list[forward_layer_idx]
-            lora_output = linear_fc1_lora_A(hidden_states)
-            lora_output= linear_fc1_lora_B(lora_output)
-            intermediate_parallel = self.linear_fc1(hidden_states)
-            intermediate_parallel = intermediate_parallel + lora_output
-        else:
-            intermediate_parallel= self.linear_fc1(hidden_states)
-
-        intermediate_parallel = self.activation_func(intermediate_parallel)
+        intermediate_parallel_up = self.linear_fc1_up(hidden_states)
+        intermediate_parallel_gate = self.linear_fc1_gate(hidden_states)
+        intermediate_parallel = F.gelu(intermediate_parallel_up) * intermediate_parallel_gate
         # [s, b, h]
         output = self.linear_fc2(intermediate_parallel)
 
@@ -892,6 +857,8 @@ class Zamba2AttentionDecoderLayer(nn.Module):
         self.feed_forward = Zamba2MLP(config, layer_idx=-1, num_mem_blocks = num_gs)
         self.input_layernorm = Zamba2RMSNorm(2 * config.hidden_size, eps=config.rms_norm_eps)
         self.pre_ff_layernorm = Zamba2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        self.mask_self_attn = False
+        self.mask_feed_forward = False
 
     # The argument original_hidden_states is concatenated with hidden_states (which is the output of the previous (mamba) layer)
     # The concatenated tensor is then used as input of the pre-attention RMSNorm (see fig. 2 in https://arxiv.org/pdf/2405.16712).
@@ -925,14 +892,16 @@ class Zamba2AttentionDecoderLayer(nn.Module):
             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
                 Indices depicting the position of the input sequence tokens in the sequence.
         """
-        hidden_states = torch.concatenate([hidden_states, original_hidden_states], dim=-1)
+        if not self.mask_self_attn:
+            hidden_states = torch.concatenate([hidden_states, original_hidden_states], dim=-1)
+            
+            hidden_states = self.input_layernorm(hidden_states)
+            hidden_states, self_attn_weights, present_key_value = self.self_attn(hidden_states, attention_mask=attention_mask, past_key_value=past_key_value, position_ids=position_ids, layer_idx=layer_idx)
         
-        hidden_states = self.input_layernorm(hidden_states)
-        hidden_states, self_attn_weights, present_key_value = self.self_attn(hidden_states, attention_mask=attention_mask, past_key_value=past_key_value, position_ids=position_ids, layer_idx=layer_idx)
-
-        # feed-forward (MLP)
-        hidden_states = self.pre_ff_layernorm(hidden_states)
-        hidden_states = self.feed_forward(hidden_states, forward_layer_idx=layer_idx)
+        if not self.mask_feed_forward:
+            # feed-forward (MLP)
+            hidden_states = self.pre_ff_layernorm(hidden_states)
+            hidden_states = self.feed_forward(hidden_states, forward_layer_idx=layer_idx)
 
         outputs = (hidden_states,)
 
@@ -952,6 +921,7 @@ class Zamba2MambaDecoderLayer(nn.Module):
         self.mamba = Mamba2Layer(config=config, layer_idx=layer_idx, **factory_kwargs)
         self.input_layernorm = Zamba2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
         self.layer_idx = layer_idx
+        self.mask_mamba_block = False
 
     def forward(
         self,
@@ -986,17 +956,18 @@ class Zamba2MambaDecoderLayer(nn.Module):
         hidden_states = (
             hidden_states + transformer_hidden_states if transformer_hidden_states is not None else hidden_states
         )
-        hidden_states = self.input_layernorm(hidden_states)
-        hidden_states = self.mamba(
-            u=hidden_states,
-            inference_params=past_key_value,
-            attention_mask=attention_mask,
-        )
-
-        self_attn_weights = None
-
-        # residual connection after mamba
-        hidden_states = residual + hidden_states
+        if not self.mask_mamba_block:
+            hidden_states = self.input_layernorm(hidden_states)
+            hidden_states = self.mamba(
+                u=hidden_states,
+                inference_params=past_key_value,
+                attention_mask=attention_mask,
+            )
+    
+            self_attn_weights = None
+    
+            # residual connection after mamba
+            hidden_states = residual + hidden_states
 
         outputs = (hidden_states,)
 
@@ -1144,7 +1115,7 @@ class Zamba2Model(Zamba2PreTrainedModel):
                 )
 
         self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
-        self.blocks = torch.nn.ModuleList([Zamba2AttentionDecoderLayer(config) for _ in range(config.num_mem_blocks)])
+        self.blocks = torch.nn.ModuleList([Zamba2AttentionDecoderLayer(config) for _ in range(count_mem_blocks_in_config(config))])
         mamba_layers = []
         linear_layers = []
         self.layers_block_type = config.layers_block_type
@@ -1234,7 +1205,7 @@ class Zamba2Model(Zamba2PreTrainedModel):
             if layer_type == "g":
                 if self.gradient_checkpointing and self.training:
                     layer_outputs = self._gradient_checkpointing_func(
-                        self.blocks[block_count % self.config.num_mem_blocks].__call__,
+                        self.blocks[block_count].__call__,
                         hidden_states,
                         original_hidden_states,
                         block_count,
@@ -1247,7 +1218,7 @@ class Zamba2Model(Zamba2PreTrainedModel):
                         block_count,
                     )
                 else:
-                    layer_outputs = self.blocks[block_count % self.config.num_mem_blocks](
+                    layer_outputs = self.blocks[block_count](
                         hidden_states,
                         original_hidden_states=original_hidden_states,
                         layer_idx=block_count,
