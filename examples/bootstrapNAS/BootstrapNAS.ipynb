{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "git68adWeq4l"
   },
   "source": [
    "<h1 style=\"font-size: 22px; line-height: 100%; text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142); border-radius: 10px;\">\n",
    "Automated Neural Architecture Search with BootstrapNAS\n",
    "</h1>\n",
    "\n",
    "This notebook demonstrates how to use [BootstrapNAS](https://arxiv.org/abs/2112.10878), a capability in NNCF to generate weight-sharing super-networks from pre-trained models. Once the super-network has been generated, BootstrapNAS can train it and search for efficient sub-networks. \n",
    "\n",
    "\\*BootstrapNAS is an **experimental feature** in NNCF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"./docs/architecture.png\" alt=\"BootstrapNAS Architecture\" width=\"90%\"/>        \n",
    "</p>\n",
    "\n",
    "As illustrated in the figure above, BootstrapNAS (1) takes as input a pre-trained model. (2) It uses this model to generate a weight-sharing super-network. (3) BootstrapNAS then applies a training strategy, and once the super-network has been trained, (4) it searches for efficient subnetworks that satisfy the user's requirements. (5) The configuration of the discovered sub-network(s) is returned to the user.\n",
    "\n",
    "We will use [ResNet-50](https://arxiv.org/abs/1512.03385) as input pre-trained model. Our goal is to discover alternative models, a.k.a., subnetworks, that are more efficient than the input pre-trained model. In this example, the model has been trained with CIFAR-10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6M1xndNu-z_2"
   },
   "source": [
    "<h3 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\">Imports and Settings\n",
    "</h3>\n",
    "\n",
    "First, we need to import NNCF and all auxiliary packages to our Python code. Set a name for the model, and the image width and height that will be used for the network. Also define paths where the output files will be stored. \n",
    "<!-- > NOTE: All NNCF logging messages below ERROR level (INFO and WARNING) are disabled to simplify the tutorial. For production use, it is recommended to enable logging, by removing ```set_log_level(logging.ERROR)```. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BtaM_i2mEB0z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported PyTorch and NNCF\n"
     ]
    }
   ],
   "source": [
    "from imports_bnas import * # Import NNCF, PyTorch and other required packages.   \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_DIR = Path(\"model\")\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "DATA_DIR = Path(\"data\")\n",
    "BASE_MODEL_NAME = \"resnet-50\"\n",
    "\n",
    "fp32_pth_path, model_onnx_path, supernet_onnx_path, subnet_onnx_path = create_folders_demo(BASE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "    Download the Weights for the Pre-trained Model\n",
    "</h3>\n",
    "    \n",
    "We obtain the weights for the pre-trained ResNet-50 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'model/resnet-50_fp32.pth' already exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/mbeale/projects/ForAutoXRepo/model/resnet-50_fp32.pth')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_on_cifar10 = True\n",
    "fp32_pth_url = \"http://hsw1.jf.intel.com/share/bootstrapNAS/checkpoints/cifar10/resnet50.pt\"\n",
    "download_file(fp32_pth_url, directory=MODEL_DIR, filename=fp32_pth_path.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIo5S145S0Ug",
    "outputId": "9a2db892-eb38-4863-dfdb-560aa12c8232",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "Prepare the Dataset\n",
    "</h3>\n",
    "    \n",
    "Next, prepare the CIFAR-10 dataset. The CIFAR-10 dataset contains:\n",
    "* 60,000 images of shape 3x32x32\n",
    "* 10 different classes (6,000 images per class): airplane, automobile, etc. \n",
    "\n",
    "Here, the dataloader is created for both the training and validation dataset which includes normalization, crop, and other transformation.  Each dataloader uses 4 workers and a batch size of 64 for training and 1000 for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-HxsU71bEbLS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = DATA_DIR / \"cifar10\"\n",
    "\n",
    "batch_size_val = 1000\n",
    "batch_size = 64\n",
    "train_loader, val_loader = create_cifar10_dataloader(DATASET_DIR, batch_size, batch_size_val, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZX2GAh3W7ZT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 5px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "Generate a super-network from a pre-trained model\n",
    "</h1>\n",
    " \n",
    "<!-- Using NNCF for model compression assumes that the user has a pre-trained model and a training pipeline. Next, we demonstrate one possible training pipeline. -->\n",
    "\n",
    "<h3 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "Load the Pre-trained Model\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet50_cifar10()\n",
    "state_dict = torch.load(\"model/resnet-50_fp32.pth\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "    Evaluate the Pre-trained Model\n",
    "</h3>\n",
    "    \n",
    "    We want to evaluate the pre-trained model using the validation dataset to have a reference of how the model performs, and then to use BootstrapNAS to discover an alternative model that might outperform this input model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 0/10]\tTime 20.897 (20.897)\tLoss 0.234 (0.234)\tAcc@1 93.70 (93.70)\tAcc@5 99.90 (99.90)\n",
      " * Acc@1 93.650 Acc@5 99.810\n"
     ]
    }
   ],
   "source": [
    "model_top1_acc, _, _ = validate(model, val_loader) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input pre-trained model has a top 1 accuracy of **93.66%**. We will use BootstrapNAS to find a sub-network with similar accuracy but more efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "Generate and Train the Super-network\n",
    "</h3>\n",
    "\n",
    "We use the pre-trained MobileNet-V2 model to generate a weight-sharing super-network. As illustrated below, BootstrapNAS will automatically insert hooks that wrap the original model operators.\n",
    "\n",
    "<img src=\"./docs/wrap_op.png\" alt=\"Operator\" width=\"400\"/>\n",
    "\n",
    "Next, we define a configuration that contains details about the super-network desired structure and how it will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "            \"device\": device,\n",
    "            \"input_info\": {\n",
    "                \"sample_size\": [1, 3, 32, 32],\n",
    "            },\n",
    "            \"checkpoint_save_dir\": OUTPUT_DIR,\n",
    "            \"bootstrapNAS\": {\n",
    "                \"training\": {\n",
    "                    \"batchnorm_adaptation\": {\n",
    "                        \"num_bn_adaptation_samples\": 2\n",
    "                    },\n",
    "                    \"schedule\": {\n",
    "                        \"list_stage_descriptions\": [\n",
    "                            {\"train_dims\": [\"depth\"], \"epochs\": 1, \"init_lr\": 2.5e-6},\n",
    "                        ]\n",
    "                    },\n",
    "                    \"elasticity\": {\n",
    "                        \"available_elasticity_dims\": [\"width\", \"depth\"]\n",
    "                    },\n",
    "                },\n",
    "                \"search\": {\n",
    "                    \"algorithm\": \"NSGA2\",\n",
    "                    \"num_evals\": 5, \n",
    "                    \"population\": 2, \n",
    "                    \"ref_acc\": model_top1_acc.item(),\n",
    "                    \"acc_delta\": 4\n",
    "                }\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[0][  0/781]\tTime 11.321 (11.321)\tLoss 1.643 (1.643)\tAcc@1 60.94 (60.94)\tAcc@5 89.06 (89.06)\n",
      "Test: [ 0/10]\tTime 16.876 (16.876)\tLoss 0.912 (0.912)\tAcc@1 86.30 (86.30)\tAcc@5 99.20 (99.20)\n",
      " * Acc@1 86.840 Acc@5 98.690\n",
      "Test: [ 0/10]\tTime 19.821 (19.821)\tLoss 0.316 (0.316)\tAcc@1 92.80 (92.80)\tAcc@5 99.60 (99.60)\n",
      " * Acc@1 92.940 Acc@5 99.640\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate and train the super-network\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    nncf_config = NNCFConfig.from_dict(config)\n",
    "\n",
    "    bn_adapt_args = BNAdaptationInitArgs(data_loader=wrap_dataloader_for_init(train_loader), device=device)\n",
    "    nncf_config.register_extra_structs([bn_adapt_args])\n",
    "  \n",
    "    # Super-network generation\n",
    "    nncf_network = create_nncf_network(model, nncf_config)\n",
    "\n",
    "    # Training\n",
    "    train_steps = 10\n",
    "    def train_epoch_fn(loader, model, compression_ctrl, epoch, optimizer):\n",
    "        train_epoch(loader, model, device, criterion, optimizer, epoch, compression_ctrl, train_iters=train_steps)\n",
    "\n",
    "    training_algorithm = EpochBasedTrainingAlgorithm.from_config(nncf_network, nncf_config)\n",
    "    nncf_network, elasticity_ctrl = training_algorithm.run(train_epoch_fn, train_loader,\n",
    "                                                           validate, val_loader, optimizer,\n",
    "                                                           OUTPUT_DIR, None,\n",
    "                                                           train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 5px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "Search for Efficient Sub-networks\n",
    "</h1>\n",
    "\n",
    "Once the super-network has been trained, we use NSGA2, as specified in the configuration above, to search for efficient sub-networks that satisfy the user's requirements. The configuration of the discovered sub-network(s) and ther performance metrics will be returned to the user. In addition to NSGA2, the user could implement her own search algorithm using any other multi-objective optimization approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiled modules for significant speedup can not be used!\n",
      "https://pymoo.org/installation.html#installation\n",
      "\n",
      "To disable this warning:\n",
      "from pymoo.config import Config\n",
      "Config.show_compile_hint = False\n",
      "\n",
      "Test: [ 0/10]\tTime 20.195 (20.195)\tLoss 0.316 (0.316)\tAcc@1 92.80 (92.80)\tAcc@5 99.60 (99.60)\n",
      " * Acc@1 92.940 Acc@5 99.640\n",
      "Test: [ 0/10]\tTime 18.701 (18.701)\tLoss 0.341 (0.341)\tAcc@1 91.30 (91.30)\tAcc@5 99.50 (99.50)\n",
      " * Acc@1 91.830 Acc@5 99.610\n",
      "Test: [ 0/10]\tTime 18.390 (18.390)\tLoss 0.289 (0.289)\tAcc@1 92.50 (92.50)\tAcc@5 99.60 (99.60)\n",
      " * Acc@1 92.790 Acc@5 99.750\n",
      "=======================================================\n",
      "n_gen |  n_eval |  n_nds  |     eps      |  indicator  \n",
      "=======================================================\n",
      "    1 |       2 |       2 |            - |            -\n",
      "Test: [ 0/10]\tTime 15.752 (15.752)\tLoss 0.527 (0.527)\tAcc@1 88.20 (88.20)\tAcc@5 99.60 (99.60)\n",
      " * Acc@1 88.610 Acc@5 99.420\n",
      "Test: [ 0/10]\tTime 17.968 (17.968)\tLoss 0.239 (0.239)\tAcc@1 92.50 (92.50)\tAcc@5 99.90 (99.90)\n",
      " * Acc@1 92.860 Acc@5 99.780\n",
      "    2 |       4 |       2 |  1.000000000 |        ideal\n",
      "Best config: OrderedDict([(<ElasticityDim.DEPTH: 'depth'>, [4])])\n",
      "Performance metrics: [307.974144, 91.83000183105469]\n"
     ]
    }
   ],
   "source": [
    "search_algo = SearchAlgorithm.from_config(nncf_network, elasticity_ctrl, nncf_config)\n",
    "\n",
    "def validate_model_fn_top1(model, val_loader):\n",
    "    top1, _, _ = validate(model, val_loader, criterion)\n",
    "    return top1.item()\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    elasticity_ctrl, best_config, performance_metrics = search_algo.run(validate_model_fn_top1, val_loader,\n",
    "                                                                        OUTPUT_DIR,\n",
    "                                                                        tensorboard_writer=None)\n",
    "\n",
    "print(\"Best config: {best_config}\".format(best_config=best_config))\n",
    "print(\"Performance metrics: {performance_metrics}\".format(performance_metrics=performance_metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "Visualization of the Search Stage\n",
    "</h3>    \n",
    "    \n",
    "After the search has concluded, we can visualize the search progression phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_algo.visualize_search_progression(filename=Path(OUTPUT_DIR / (BASE_MODEL_NAME + \"_search\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we have used just a few epochs to train the super-network. Please refer to the `schedule` section in the configuration file. Increasing the number of training epochs and the elasticity of the super-newtwork will result in a greater search space from which we can extract efficient sub-networks. For instance, this is the result that we get by increasing the number of epochs and the elasticity of the super-network: \n",
    "\n",
    "\n",
    "<img src=\"./docs/search_progression.png\" alt=\"BootstrapNAS ResNet-50\" width=\"600\" align=\"center\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated in the figure above, many sub-networks are more efficient than the original pre-trained model, and the user can choose whether to tolerate a drop in accuracy for more efficiency.\n",
    "\n",
    "<h3 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "Summary\n",
    "</h3>\n",
    "\n",
    "In this notebook, we have demonstrated NNCF's BootstrapNAS capability to generate and train weight-sharing super-networks, and the subsequent discovery of efficient sub-networks. BootstrapNAS is available at the [NNCF repository](https://github.com/openvinotoolkit/nncf/blob/develop/examples/experimental/torch/classification/Quickstart.md). "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "K5HPrY_d-7cV",
    "E01dMaR2_AFL",
    "qMnYsGo9_MA8",
    "L0tH9KdwtHhV"
   ],
   "name": "NNCF Quantization PyTorch Demo (tiny-imagenet/resnet-18)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b460384b52104c1e5b9cf54bee46a255d22b2bef338f75ac4ad5d48196028d3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
