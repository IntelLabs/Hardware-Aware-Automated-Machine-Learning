{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "git68adWeq4l",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1 style=\"font-size: 22px; line-height: 100%; text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142); border-radius: 10px;\">\n",
    "Automated Neural Architecture Search with BootstrapNAS\n",
    "</h1>\n",
    "\n",
    "This notebook demonstrates how to use [BootstrapNAS](https://arxiv.org/abs/2112.10878), a capability in NNCF to generate weight-sharing super-networks from pre-trained models. Once the super-network has been generated, BootstrapNAS can train it and search for efficient sub-networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"./imgs/architecture.png\" alt=\"BootstrapNAS Architecture\" width=\"90%\"/>\n",
    "</p>\n",
    "\n",
    "As illustrated in the figure above, BootstrapNAS (1) takes as input a pre-trained model. (2) It uses this model to generate a weight-sharing super-network. (3) BootstrapNAS then applies a training strategy, and once the super-network has been trained, (4) it searches for efficient subnetworks that satisfy the user's requirements. (5) The configuration of the discovered sub-network(s) is returned to the user.\n",
    "\n",
    "We will use [ResNet-50](https://arxiv.org/abs/1512.03385) as input pre-trained model. Our goal is to discover alternative models, a.k.a., subnetworks, that are more efficient than the input pre-trained model. In this example, the model has been trained with CIFAR-10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6M1xndNu-z_2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\">Imports and Settings\n",
    "</h3>\n",
    "\n",
    "First, we need to import NNCF and all auxiliary packages to our Python code. Set a name for the model, and the image width and height that will be used for the network. Also define paths where the output files will be stored. \n",
    "<!-- > NOTE: All NNCF logging messages below ERROR level (INFO and WARNING) are disabled to simplify the tutorial. For production use, it is recommended to enable logging, by removing ```set_log_level(logging.ERROR)```. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BtaM_i2mEB0z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported PyTorch and NNCF\n"
     ]
    }
   ],
   "source": [
    "from imports_bnas import * # Import NNCF, PyTorch and other required packages.   \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_DIR = Path(\"../../models/pretrained\")\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "DATA_DIR = Path(\"data\")\n",
    "BASE_MODEL_NAME = \"resnet-50\"\n",
    "\n",
    "fp32_pth_path, model_onnx_path, supernet_onnx_path, subnet_onnx_path = create_folders_demo(BASE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "    Download the Weights for the Pre-trained Model\n",
    "</h3>\n",
    "    \n",
    "You can obtain weights for several models trained with CIFAR-10 from this [Git repository](https://github.com/huyvnphan/PyTorch_CIFAR10). For your convenience, we have extracted the weight for ResNet-50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fp32_pth_url = \"../../models/pretrained/resnet50.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIo5S145S0Ug",
    "outputId": "9a2db892-eb38-4863-dfdb-560aa12c8232",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "Prepare the Dataset\n",
    "</h3>\n",
    "    \n",
    "Next, prepare the CIFAR-10 dataset. The CIFAR-10 dataset contains:\n",
    "* 60,000 images of shape 3x32x32\n",
    "* 10 different classes (6,000 images per class): airplane, automobile, etc. \n",
    "\n",
    "Here, the dataloader is created for both the training and validation dataset which includes normalization, crop, and other transformation.  Each dataloader uses 4 workers and a batch size of 64 for training and 1000 for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-HxsU71bEbLS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = DATA_DIR / \"cifar10\"\n",
    "\n",
    "batch_size_val = 1000\n",
    "batch_size = 64\n",
    "train_loader, val_loader = create_cifar10_dataloader(DATASET_DIR, batch_size, batch_size_val, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZX2GAh3W7ZT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 5px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "Generate a super-network from a pre-trained model\n",
    "</h1>\n",
    " \n",
    "<!-- Using NNCF for model compression assumes that the user has a pre-trained model and a training pipeline. Next, we demonstrate one possible training pipeline. -->\n",
    "\n",
    "<h3 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "Load the Pre-trained Model\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet50_cifar10()\n",
    "state_dict = torch.load(fp32_pth_url)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "    Evaluate the Pre-trained Model\n",
    "</h3>\n",
    "    \n",
    "    We want to evaluate the pre-trained model using the validation dataset to have a reference of how the model performs, and then to use BootstrapNAS to discover an alternative model that might outperform this input model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 0/10]\tTime 0.562 (0.562)\tLoss 0.234 (0.234)\tAcc@1 93.70 (93.70)\tAcc@5 99.90 (99.90)\n",
      " * Acc@1 93.660 Acc@5 99.810\n"
     ]
    }
   ],
   "source": [
    "model_top1_acc, _, _ = validate(model, val_loader) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The input pre-trained model has a top 1 accuracy of **93.66%**. We will use BootstrapNAS to find a sub-network with similar accuracy but more efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "Generate and Train the Super-network\n",
    "</h3>\n",
    "\n",
    "We use the pre-trained MobileNet-V2 model to generate a weight-sharing super-network. We define a configuration that contains details about the super-network desired structure and how it will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "            \"device\": device,\n",
    "            \"input_info\": {\n",
    "                \"sample_size\": [1, 3, 32, 32],\n",
    "            },\n",
    "            \"checkpoint_save_dir\": OUTPUT_DIR,\n",
    "            \"bootstrapNAS\": {\n",
    "                \"training\": {\n",
    "                    \"batchnorm_adaptation\": {\n",
    "                        \"num_bn_adaptation_samples\": 2\n",
    "                    },\n",
    "                    \"schedule\": {\n",
    "                        \"list_stage_descriptions\": [\n",
    "                            {\"train_dims\": [\"depth\"], \"epochs\": 1, \"init_lr\": 2.5e-6},\n",
    "                        ]\n",
    "                    },\n",
    "                    \"elasticity\": {\n",
    "                        \"available_elasticity_dims\": [\"width\", \"depth\"]\n",
    "                    },\n",
    "                },\n",
    "                \"search\": {\n",
    "                    \"algorithm\": \"NSGA2\",\n",
    "                    \"num_evals\": 5, \n",
    "                    \"population\": 2, \n",
    "                    \"ref_acc\": model_top1_acc.item(),\n",
    "                    \"acc_delta\": 4\n",
    "                }\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called Initialize\n",
      "Epoch:[0][  0/781]\tTime 0.681 (0.681)\tLoss 1.084 (1.084)\tAcc@1 65.62 (65.62)\tAcc@5 93.75 (93.75)\n",
      "Test: [ 0/10]\tTime 0.425 (0.425)\tLoss 0.938 (0.938)\tAcc@1 85.70 (85.70)\tAcc@5 99.20 (99.20)\n",
      " * Acc@1 86.130 Acc@5 98.650\n",
      "Test: [ 0/10]\tTime 0.432 (0.432)\tLoss 0.303 (0.303)\tAcc@1 92.90 (92.90)\tAcc@5 99.60 (99.60)\n",
      " * Acc@1 93.100 Acc@5 99.690\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate and train the super-network\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    nncf_config = NNCFConfig.from_dict(config)\n",
    "\n",
    "    bn_adapt_args = BNAdaptationInitArgs(data_loader=wrap_dataloader_for_init(train_loader), device=device)\n",
    "    nncf_config.register_extra_structs([bn_adapt_args])\n",
    "  \n",
    "    # Super-network generation\n",
    "    nncf_network = create_nncf_network(model, nncf_config)\n",
    "\n",
    "    # Training\n",
    "    train_steps = 10\n",
    "    def train_epoch_fn(loader, model, compression_ctrl, epoch, optimizer):\n",
    "        train_epoch(loader, model, device, criterion, optimizer, epoch, compression_ctrl, train_iters=train_steps)\n",
    "\n",
    "    training_algorithm = EpochBasedTrainingAlgorithm.from_config(nncf_network, nncf_config)\n",
    "    nncf_network, elasticity_ctrl = training_algorithm.run(train_epoch_fn, train_loader,\n",
    "                                                           validate, val_loader, optimizer,\n",
    "                                                           OUTPUT_DIR, None,\n",
    "                                                           train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 5px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "Search for Efficient Sub-networks\n",
    "</h1>\n",
    "\n",
    "Once the super-network has been trained, we use NSGA2, as specified in the configuration above, to search for efficient sub-networks that satisfy the user's requirements. The configuration of the discovered sub-network(s) and ther performance metrics will be returned to the user. In addition to NSGA2, the user could implement her own search algorithm using any other multi-objective optimization approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 0/10]\tTime 0.442 (0.442)\tLoss 0.303 (0.303)\tAcc@1 92.90 (92.90)\tAcc@5 99.60 (99.60)\n",
      " * Acc@1 93.100 Acc@5 99.690\n",
      "Test: [ 0/10]\tTime 0.464 (0.464)\tLoss 0.340 (0.340)\tAcc@1 91.20 (91.20)\tAcc@5 99.40 (99.40)\n",
      " * Acc@1 91.780 Acc@5 99.580\n",
      "Test: [ 0/10]\tTime 0.462 (0.462)\tLoss 0.290 (0.290)\tAcc@1 92.40 (92.40)\tAcc@5 99.60 (99.60)\n",
      " * Acc@1 92.850 Acc@5 99.750\n",
      "=======================================================\n",
      "n_gen |  n_eval |  n_nds  |     eps      |  indicator  \n",
      "=======================================================\n",
      "    1 |       2 |       2 |            - |            -\n",
      "Test: [ 0/10]\tTime 0.430 (0.430)\tLoss 0.526 (0.526)\tAcc@1 88.10 (88.10)\tAcc@5 99.60 (99.60)\n",
      " * Acc@1 88.640 Acc@5 99.410\n",
      "Test: [ 0/10]\tTime 0.440 (0.440)\tLoss 0.238 (0.238)\tAcc@1 92.60 (92.60)\tAcc@5 99.90 (99.90)\n",
      " * Acc@1 92.870 Acc@5 99.780\n",
      "    2 |       4 |       2 |  1.000000000 |        ideal\n",
      "Best config: OrderedDict([(<ElasticityDim.DEPTH: 'depth'>, [4])])\n",
      "Performance metrics: [307.974144, 91.77999877929688]\n"
     ]
    }
   ],
   "source": [
    "search_algo = SearchAlgorithm.from_config(nncf_network, elasticity_ctrl, nncf_config)\n",
    "\n",
    "def validate_model_fn_top1(model, val_loader):\n",
    "    top1, _, _ = validate(model, val_loader, criterion)\n",
    "    return top1.item()\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    elasticity_ctrl, best_config, performance_metrics = search_algo.run(validate_model_fn_top1, val_loader,\n",
    "                                                                        OUTPUT_DIR,\n",
    "                                                                        tensorboard_writer=None)\n",
    "\n",
    "print(\"Best config: {best_config}\".format(best_config=best_config))\n",
    "print(\"Performance metrics: {performance_metrics}\".format(performance_metrics=performance_metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "Visualization of the Search Stage\n",
    "</h3>    \n",
    "    \n",
    "After the search has concluded, we can visualize the search progression phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_algo.visualize_search_progression(filename=Path(OUTPUT_DIR / (BASE_MODEL_NAME + \"_search\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the example above, we have used just a few epochs to train the super-network. Please refer to the `schedule` section in the configuration file. Increasing the number of training epochs and the elasticity of the super-newtwork will result in a greater search space from which we can extract efficient sub-networks. For instance, this is the result that we get by increasing the number of epochs and the elasticity of the super-network: \n",
    "\n",
    "\n",
    "<img src=\"./imgs/search_progression.png\" alt=\"BootstrapNAS ResNet-50\" width=\"600\" align=\"center\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As illustrated in the figure above, many sub-networks are more efficient than the original pre-trained model, and the user can choose whether to tolerate a drop in accuracy for more efficiency.\n",
    "\n",
    "<h3 style=\"text-align: center; background-color: rgb(36, 24, 142); color: white; border: 4px solid rgb(36, 24, 142);\n",
    "border-radius: 25px;\n",
    "\">\n",
    "Summary\n",
    "</h3>\n",
    "\n",
    "In this notebook, we have demonstrated NNCF's BootstrapNAS capability to generate and train weight-sharing super-networks, and the subsequent discovery of efficient sub-networks. BootstrapNAS is available at the [NNCF repository](https://github.com/openvinotoolkit/nncf/blob/develop/examples/experimental/torch/classification/Quickstart.md). "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "K5HPrY_d-7cV",
    "E01dMaR2_AFL",
    "qMnYsGo9_MA8",
    "L0tH9KdwtHhV"
   ],
   "name": "NNCF Quantization PyTorch Demo (tiny-imagenet/resnet-18)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Intel Environment",
   "language": "python",
   "name": "test_wheel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "b460384b52104c1e5b9cf54bee46a255d22b2bef338f75ac4ad5d48196028d3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
