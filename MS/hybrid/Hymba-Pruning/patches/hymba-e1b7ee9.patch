diff --git a/modeling_hymba.py b/modeling_hymba.py
index 182b4a6..90dec28 100644
--- a/modeling_hymba.py
+++ b/modeling_hymba.py
@@ -1561,6 +1561,9 @@ class HymbaBlock(nn.Module):
                 " is None. To install follow https://github.com/state-spaces/mamba/#installation and"
                 " https://github.com/Dao-AILab/causal-conv1d. If you want to use the naive implementation, set `use_mamba_kernels=False` in the model config"
             )
+        
+        self.mask_self_attn = False
+        self.mask_ssm = False
 
     def set_attn_mamba_mask(self, attn_branch_mask, mamba_branch_mask):
         self.attn_branch_mask = attn_branch_mask
@@ -1648,75 +1651,89 @@ class HymbaBlock(nn.Module):
                 hidden_states = causal_conv1d_fn(
                     hidden_states, conv_weights, self.conv1d.bias, activation=self.activation
                 )
-            
             if self.reuse_kv:
                 assert kv_last_layer is not None
                 attn_outputs, attn_key_value = self.self_attn(attention_mask=attention_mask, position_ids=position_ids, query_states=query_states, kv_last_layer=kv_last_layer, use_swa=use_swa, use_cache=use_cache, past_key_value=cache_params)
             else:
                 attn_outputs, attn_key_value = self.self_attn(attention_mask=attention_mask, position_ids=position_ids, query_states=query_states, key_states=key_states, value_states=value_states, use_swa=use_swa, use_cache=use_cache, past_key_value=cache_params)
+            
+            if self.mask_self_attn:
+                attn_outputs = None
 
             ## Mamba head
-            index = 0
-            ssm_parameters = self.x_proj[index](hidden_states.transpose(1, 2))
-            time_step, B, C = torch.split(
-                ssm_parameters, [self.time_step_rank, self.ssm_state_size, self.ssm_state_size], dim=-1
-            )
-            time_step, B, C = self._apply_layernorms(time_step, B, C)
-
-            if hasattr(self.dt_proj[index], "base_layer"):
-                time_proj_bias = self.dt_proj[index].base_layer.bias
-                self.dt_proj[index].base_layer.bias = None
-            else:
-                time_proj_bias = self.dt_proj[index].bias
-                self.dt_proj[index].bias = None
-            discrete_time_step = self.dt_proj[index](time_step).transpose(1, 2)  # [batch, intermediate_size, seq_len]
-
-            if hasattr(self.dt_proj[index], "base_layer"):
-                self.dt_proj[index].base_layer.bias = time_proj_bias
-            else:
-                self.dt_proj[index].bias = time_proj_bias
-
-            A = -torch.exp(self.A_log[index].float())
-
-            time_proj_bias = time_proj_bias.float() if time_proj_bias is not None else None
-            if use_precomputed_states:
-                scan_outputs = selective_state_update(
-                    cache_params.ssm_states[self.layer_idx],
-                    hidden_states[..., 0],
-                    discrete_time_step[..., 0],
-                    A,
-                    B[:, 0],
-                    C[:, 0],
-                    self.D[index],
-                    gate[..., 0],
-                    time_proj_bias,
-                    dt_softplus=True,
-                ).unsqueeze(-1)
+            if self.mask_ssm:
+                scan_outputs = None
             else:
-                outputs = selective_scan_fn(
-                    hidden_states,
-                    discrete_time_step,
-                    A,
-                    B.transpose(1, 2),
-                    C.transpose(1, 2),
-                    self.D[index].float(),
-                    z=gate,
-                    delta_bias=time_proj_bias,
-                    delta_softplus=True,
-                    return_last_state=True,
+                index = 0
+                ssm_parameters = self.x_proj[index](hidden_states.transpose(1, 2))
+                time_step, B, C = torch.split(
+                    ssm_parameters, [self.time_step_rank, self.ssm_state_size, self.ssm_state_size], dim=-1
                 )
-                
-                if len(outputs) == 3:
-                    scan_outputs, ssm_state, _ = outputs
+                time_step, B, C = self._apply_layernorms(time_step, B, C)
+
+                if hasattr(self.dt_proj[index], "base_layer"):
+                    time_proj_bias = self.dt_proj[index].base_layer.bias
+                    self.dt_proj[index].base_layer.bias = None
                 else:
-                    scan_outputs, ssm_state = outputs
+                    time_proj_bias = self.dt_proj[index].bias
+                    self.dt_proj[index].bias = None
+                discrete_time_step = self.dt_proj[index](time_step).transpose(1, 2)  # [batch, intermediate_size, seq_len]
 
-                if ssm_state is not None and cache_params is not None:
-                    cache_params.ssm_states[self.layer_idx].copy_(ssm_state)
+                if hasattr(self.dt_proj[index], "base_layer"):
+                    self.dt_proj[index].base_layer.bias = time_proj_bias
+                else:
+                    self.dt_proj[index].bias = time_proj_bias
+
+                A = -torch.exp(self.A_log[index].float())
+                B = B.to(hidden_states.dtype)
+                C = C.to(hidden_states.dtype)
+
+                time_proj_bias = time_proj_bias.float() if time_proj_bias is not None else None
+                if use_precomputed_states:
+                    scan_outputs = selective_state_update(
+                        cache_params.ssm_states[self.layer_idx],
+                        hidden_states[..., 0],
+                        discrete_time_step[..., 0],
+                        A,
+                        B[:, 0],
+                        C[:, 0],
+                        self.D[index],
+                        gate[..., 0],
+                        time_proj_bias,
+                        dt_softplus=True,
+                    ).unsqueeze(-1)
+                else:
+                    outputs = selective_scan_fn(
+                        hidden_states,
+                        discrete_time_step,
+                        A,
+                        B.transpose(1, 2),
+                        C.transpose(1, 2),
+                        self.D[index].float(),
+                        z=gate,
+                        delta_bias=time_proj_bias,
+                        delta_softplus=True,
+                        return_last_state=True,
+                    )
                     
-            scan_outputs = scan_outputs.transpose(1, 2)
-
-            hidden_states = (self.pre_avg_layernorm1(attn_outputs) + self.pre_avg_layernorm2(scan_outputs)) / 2
+                    if len(outputs) == 3:
+                        scan_outputs, ssm_state, _ = outputs
+                    else:
+                        scan_outputs, ssm_state = outputs
+
+                    if ssm_state is not None and cache_params is not None:
+                        cache_params.ssm_states[self.layer_idx].copy_(ssm_state)
+                        
+                scan_outputs = scan_outputs.transpose(1, 2)
+
+            if attn_outputs is None and scan_outputs is None:
+                hidden_states = hidden_states.transpose(1, 2)
+            elif attn_outputs is None:
+                hidden_states = self.pre_avg_layernorm2(scan_outputs)
+            elif scan_outputs is None:
+                hidden_states = self.pre_avg_layernorm1(attn_outputs)
+            else:
+                hidden_states = (self.pre_avg_layernorm1(attn_outputs) + self.pre_avg_layernorm2(scan_outputs)) / 2
             contextualized_states = self.out_proj(hidden_states)
 
         return contextualized_states, attn_key_value
@@ -1886,6 +1903,9 @@ class HymbaDecoderLayer(nn.Module):
             self.moe = HymbaSparseMoeBlock(config, num_experts=num_experts, num_experts_per_tok=num_experts_per_tok)
 
             self.pre_moe_layernorm = HymbaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        
+        self.mask_hymba_block = False
+        self.mask_moe = False # FFN
 
 
     def forward(
@@ -1923,29 +1943,34 @@ class HymbaDecoderLayer(nn.Module):
                 (see `past_key_values`).
         """
 
-        residual = hidden_states
+        
 
-        hidden_states = self.input_layernorm(hidden_states)
+        if self.mask_hymba_block:
+            attn_key_value, present_key_value = None, None
+        else:
+            residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
 
-        hidden_states, attn_key_value, present_key_value = self.mamba(
-            hidden_states=hidden_states,
-            past_key_value=past_key_value,
-            attention_mask=attention_mask,
-            position_ids=position_ids,
-            kv_last_layer=kv_last_layer,
-            use_cache=use_cache,
-            use_swa=use_swa
-        )
+            hidden_states, attn_key_value, present_key_value = self.mamba(
+                hidden_states=hidden_states,
+                past_key_value=past_key_value,
+                attention_mask=attention_mask,
+                position_ids=position_ids,
+                kv_last_layer=kv_last_layer,
+                use_cache=use_cache,
+                use_swa=use_swa
+            )
 
-        bs, seqlen, _ = hidden_states.shape
-        past_seqlen = self._get_past_seqlen(past_key_value, seqlen)
-        num_attention_heads = self.mamba.config.num_attention_heads
-        self_attn_weights = torch.empty(bs, num_attention_heads, seqlen, past_seqlen, device="meta")
+            bs, seqlen, _ = hidden_states.shape
+            past_seqlen = self._get_past_seqlen(past_key_value, seqlen)
+            num_attention_heads = self.mamba.config.num_attention_heads
+            self_attn_weights = torch.empty(bs, num_attention_heads, seqlen, past_seqlen, device="meta")
 
-        # residual connection after mamba
-        hidden_states = residual + hidden_states
+            # residual connection after mamba
+            hidden_states = residual + hidden_states
 
-        if self.intermediate_size > 0:
+        router_logits = None
+        if self.intermediate_size > 0 and not self.mask_moe:
             residual = hidden_states
             hidden_states = self.pre_moe_layernorm(hidden_states)
             hidden_states, router_logits = self.moe(hidden_states)
